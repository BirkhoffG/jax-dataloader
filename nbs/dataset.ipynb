{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "> Pytorch-like Dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import print_function, division, annotations\n",
    "from jax_dataloader.imports import *\n",
    "from jax_dataloader.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset:\n",
    "    \"\"\"A pytorch-like Dataset class.\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ArrayDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.DeviceArray # Numpy array with same first dimension\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to [torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset), \n",
    "but it wrapps numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "X = jnp.arange(10000).reshape(1000, 10)\n",
    "y = jnp.arange(1000)\n",
    "ds = ArrayDataset(X, y)\n",
    "assert len(ds) == 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index numpy arrays along the first dimension.\n",
    "Dataset indexing is done via `ds[index]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = ds[1] # get the first sample\n",
    "assert jnp.array_equal(x1, X[1])\n",
    "assert jnp.array_equal(y1, y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _has_tensor(batch) -> bool:\n",
    "    if isinstance(batch[0], torch.Tensor):\n",
    "        return True\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return any([_has_tensor(samples) for samples in transposed])\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TorchDataset(Dataset):\n",
    "    \"\"\"[Deprecated] A Dataset class that wraps a pytorch Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: torch_data.Dataset # Pytorch Dataset\n",
    "    ):\n",
    "        check_pytorch_installed()\n",
    "        if not isinstance(dataset, torch_data.Dataset):\n",
    "            raise TypeError(f\"`dataset` must be a torch Dataset, but got {type(dataset)}\")\n",
    "        # Give a warning if the dataset is not in numpy format\n",
    "        if _has_tensor(dataset[0]):\n",
    "            warnings.warn(\"The dataset contains `torch.Tensor`. \"\n",
    "                \"Please make sure the dataset is in numpy format.\")\n",
    "        self._ds = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._ds[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TorchDataset` is a wrapper class of `torch.utils.data`. It does not modify inner behavior of the input pytorch `dataset`.\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "`TorchDataset` will **NOT** turn a `torch.Tensor` into `numpy.array`.\n",
    "Therefore, it is suggested to ensure the input `dataset` is in numpy format \n",
    "before passing to the `TorchDataset`.\n",
    "`TorchDataset` will give a warning if `torch.Tensor` is found in the dataset.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset \n",
    "using the Pytorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flatten and cast the PIL image into the `numpy.array`\n",
    "(brought from [jax official tutorial](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenAndCast(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.ravel(np.array(pic, dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the pytorch [MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_torch = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can wrape the `mnist_torch` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ds = TorchDataset(mnist_torch)\n",
    "assert isinstance(mnist_ds[0][0], np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class HFDataset(Dataset):\n",
    "    \"\"\"[Deprecated] A Dataset class that wraps a huggingface Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: hf_datasets.Dataset # Huggingface Dataset\n",
    "    ):\n",
    "        check_hf_installed()\n",
    "        # if not isinstance(dataset, hf_datasets.Dataset):\n",
    "        #     raise TypeError(f\"`dataset` must be a huggingface Dataset, \"\n",
    "        #                     f\"but got {type(dataset)}\")\n",
    "        # Ensure the dataset is in jax format\n",
    "        self._ds = dataset.with_format(\"jax\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._ds[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HFDataset` wraps a huggingface dataset. Unlike `TorchDataset`,\n",
    "`HFDataset` will ensure the input dataset with the format of `jax.DeviceArray`.\n",
    "\n",
    "Again, we load the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, \n",
    "but load the data via the huggingface datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset mnist (/home/birk/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "mnist_hf = load_dataset(\"mnist\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the `mnist_hf` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ds = HFDataset(mnist_hf)\n",
    "assert isinstance(mnist_ds[0]['image'], jnp.ndarray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
