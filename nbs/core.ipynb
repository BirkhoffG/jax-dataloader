{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "> Support various dataloader for loading batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import print_function, division, annotations\n",
    "from jax_dataloader.imports import *\n",
    "from jax_dataloader.utils import *\n",
    "from jax_dataloader.datasets import *\n",
    "from jax_dataloader.loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(frozen=True)\n",
    "class DataloaderBackends:\n",
    "    jax = DataLoaderJax\n",
    "    pytorch: BaseDataLoader = DataLoaderPytorch\n",
    "    tensorflow: BaseDataLoader = None\n",
    "    merlin: BaseDataLoader = None\n",
    "\n",
    "    __all__ = dict(\n",
    "        jax=jax, pytorch=pytorch, tensorflow=tensorflow, merlin=merlin\n",
    "    )\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.__all__[key]\n",
    "\n",
    "    @property\n",
    "    def supported(self) -> List[str]:\n",
    "        return [\n",
    "            backend for backend, dl_cls in self.__all__.items() if dl_cls is not None\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_backends() -> List[str]:\n",
    "    \"\"\"Return list of supported dataloader backends\"\"\"\n",
    "    return DataloaderBackends().__all__.keys()\n",
    "\n",
    "\n",
    "def _dispatch_dataloader(\n",
    "    backend: str # dataloader backend\n",
    ") -> BaseDataLoader:\n",
    "    \"\"\"Return Dataloader class based on given `backend`\"\"\"\n",
    "    backends = DataloaderBackends()\n",
    "    if not backend in backends.supported:\n",
    "        raise ValueError(f\"backend=`{backend}` is either an invalid backend or not supported yet. \"\n",
    "            f\"Should be one of {backends.supported}.\")\n",
    "    \n",
    "    dl_cls = backends[backend]\n",
    "    return dl_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _dispatch_dataset(\n",
    "    dataset, # Dataset or Pytorch Dataset or HuggingFace Dataset\n",
    ") -> Dataset:\n",
    "    if isinstance(dataset, Dataset):\n",
    "        return dataset\n",
    "    elif is_torch_dataset(dataset):\n",
    "        # Give a warning if the dataset is not in numpy format\n",
    "        if has_pytorch_tensor(dataset[0]):\n",
    "            warnings.warn(\"The dataset contains `torch.Tensor`. \"\n",
    "                \"Please make sure the dataset is in numpy format.\")\n",
    "        return dataset\n",
    "    elif is_hf_dataset(dataset):\n",
    "        return dataset.with_format(\"jax\")\n",
    "    else:\n",
    "        raise ValueError(f\"dataset must be one of `jax_loader.Dataset`, \"\n",
    "                         \"`torch.utils.data.Dataset`, `datasets.Dataset`, \"\n",
    "                         f\"but got {type(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def is_jdl_dataset(dataset):\n",
    "    return isinstance(dataset, Dataset)\n",
    "\n",
    "\n",
    "def _check_backend_compatibility(dataset, backend: str):\n",
    "    compatible_set = {\n",
    "        \"jax\": [is_jdl_dataset, is_hf_dataset],\n",
    "        \"pytorch\": [is_jdl_dataset, is_torch_dataset, is_hf_dataset],\n",
    "        \"tensorflow\": [],\n",
    "        \"merlin\": [],\n",
    "    }\n",
    "    assert all([backend in compatible_set for backend in _get_backends()])\n",
    "\n",
    "    if not backend in _get_backends():\n",
    "        raise ValueError(f\"backend=`{backend}` is not supported yet. \"\n",
    "            f\"Should be one of {_get_backends()}.\")\n",
    "    \n",
    "    if not any([check_dataset_fn(dataset) for check_dataset_fn in compatible_set[backend]]):\n",
    "        raise ValueError(f\"dataset (type=`{type(dataset)}`) is not compatible with backend='{backend}'. \")\n",
    "    \n",
    "    # if backend != \"pytorch\" and is_torch_dataset(dataset):\n",
    "    #     raise ValueError(f\"dataset (type={type(dataset)}) is a pytorch dataset, \"\n",
    "    #                      \"which is only supported by 'pytorch' backend.\"\n",
    "    #                      f\"However, we got `backend={backend}`, which is not 'pytorch'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _dispatch_dataset_and_backend(\n",
    "    dataset, # Dataset or Pytorch Dataset or HuggingFace Dataset\n",
    "    backend: str # dataloader backend\n",
    ") -> Tuple[Dataset, BaseDataLoader]:\n",
    "    \"\"\"Return Dataset and Dataloader class based on given `dataset` and `backend`\"\"\"\n",
    "\n",
    "    # if backend != \"pytorch\" and isinstance(dataset, torch_data.Dataset):\n",
    "    #     raise ValueError(f\"dataset (type={type(dataset)}) is a pytorch dataset, \"\n",
    "    #                      \"which is only supported by 'pytorch' backend.\"\n",
    "    #                      f\"However, we got `backend={backend}`, which is not 'pytorch'.\")\n",
    "    _check_backend_compatibility(dataset, backend)\n",
    "    dataset = _dispatch_dataset(dataset)    \n",
    "    dl_cls = _dispatch_dataloader(backend)\n",
    "    return dataset, dl_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoader:\n",
    "    \"\"\"Main Dataloader class to load Numpy data batches\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset, # Dataset or Pytorch Dataset or HuggingFace Dataset\n",
    "        backend: str, # Dataloader backend\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # drop last batches or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        dataset, dl_cls = _dispatch_dataset_and_backend(dataset, backend)\n",
    "        self.dataloader = dl_cls(\n",
    "            dataset=dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            drop_last=drop_last,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader.__iter__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Minimum Example of using Dataloader\n",
    "\n",
    "We showcase how to use `Dataloader` for training a simple regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import optax\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=500, n_features=20)\n",
    "dataset = ArrayDataset(X, y.reshape(-1, 1))\n",
    "keys = hk.PRNGSequence(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `loss`, `step`, `train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, x, y):\n",
    "    return jnp.mean(vmap(optax.l2_loss)(x @ w.T, y))\n",
    "\n",
    "def step(w, x, y):\n",
    "    lr = 0.1\n",
    "    grad = jax.grad(loss)(w, x, y)\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "def train(dataloader: DataLoader, key: jax.random.PRNGKey):\n",
    "    w = jax.random.normal(key, shape=(1, 20))\n",
    "    n_epochs = 10\n",
    "    for _ in range(n_epochs):\n",
    "        for x, y in dataloader:\n",
    "            w = step(w, x, y)\n",
    "    return w\n",
    "\n",
    "def eval(dataloader: DataLoader, w):\n",
    "    err = []\n",
    "    for x, y in dataloader:\n",
    "        err.append(loss(w, x, y))\n",
    "    return np.mean(err)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderJax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'jax', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "# assert np.allclose(eval(dataloader, w), 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, 'jax', batch_size=200, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "# assert np.allclose(eval(dataloader, w), 0.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderPytorch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'pytorch', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "# assert np.allclose(eval(dataloader, w), 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
