{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "> Support various dataloader for loading batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any, Optional, Iterable, Sequence, Iterator\n",
    "import jax\n",
    "from jax import vmap, grad, jit, numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from abc import ABC\n",
    "from dataclasses import dataclass\n",
    "import collections\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try: \n",
    "    import torch.utils.data as torch_data\n",
    "    import torch\n",
    "except ModuleNotFoundError: \n",
    "    torch_data = None\n",
    "    torch = None\n",
    "\n",
    "try: import haiku as hk \n",
    "except ModuleNotFoundError: hk = None\n",
    "\n",
    "try: import datasets as hf_datasets\n",
    "except ModuleNotFoundError: hf_datasets = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "#| hide\n",
    "@dataclass\n",
    "class Config:\n",
    "    rng_reserve_size: int\n",
    "    global_seed: int\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls) -> Config:\n",
    "        return cls(rng_reserve_size=1, global_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "main_config = Config.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def get_config() -> Config:\n",
    "    return main_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PRNGSequence(Iterator[PRNGKey]):\n",
    "    \"\"\"An Interator of Jax PRNGKey (minimal version of haiku.PRNGSequence).\"\"\"\n",
    "\n",
    "    def __init__(self, seed: int):\n",
    "        self._key = jax.random.PRNGKey(seed)\n",
    "        self._subkeys = collections.deque()\n",
    "\n",
    "    def reserve(self, num):\n",
    "        \"\"\"Splits additional ``num`` keys for later use.\"\"\"\n",
    "        if num > 0:\n",
    "            new_keys = tuple(jax.random.split(self._key, num + 1))\n",
    "            self._key = new_keys[0]\n",
    "            self._subkeys.extend(new_keys[1:])\n",
    "            \n",
    "    def __next__(self):\n",
    "        if not self._subkeys:\n",
    "            self.reserve(get_config().rng_reserve_size)\n",
    "        return self._subkeys.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _check_pytorch_installed():\n",
    "    if torch_data is None:\n",
    "        raise ModuleNotFoundError(\"`pytorch` library needs to be installed. \"\n",
    "            \"Try `pip install torch`. Please refer to pytorch documentation for details: \"\n",
    "            \"https://pytorch.org/get-started/.\")\n",
    "    \n",
    "def _check_hf_installed():\n",
    "    if hf_datasets is None:\n",
    "        raise ModuleNotFoundError(\"`datasets` library needs to be installed. \"\n",
    "            \"Try `pip install datasets`. Please refer to huggingface documentation for details: \"\n",
    "            \"https://huggingface.co/docs/datasets/installation.html.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset:\n",
    "    \"\"\"A pytorch-like Dataset class.\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ArrayDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.DeviceArray # Numpy array with same first dimension\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to [torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset), \n",
    "but it wrapps numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jnp.arange(10000).reshape(1000, 10)\n",
    "y = jnp.arange(1000)\n",
    "ds = ArrayDataset(X, y)\n",
    "assert len(ds) == 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index numpy arrays along the first dimension.\n",
    "Dataset indexing is done via `ds[index]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = ds[1] # get the first sample\n",
    "assert jnp.array_equal(x1, X[1])\n",
    "assert jnp.array_equal(y1, y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _has_tensor(batch) -> bool:\n",
    "    if isinstance(batch[0], torch.Tensor):\n",
    "        return True\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return any([_has_tensor(samples) for samples in transposed])\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TorchDataset(Dataset):\n",
    "    \"\"\"A Dataset class that wraps a pytorch Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: torch_data.Dataset # Pytorch Dataset\n",
    "    ):\n",
    "        _check_pytorch_installed()\n",
    "        if not isinstance(dataset, torch_data.Dataset):\n",
    "            raise TypeError(f\"`dataset` must be a torch Dataset, but got {type(dataset)}\")\n",
    "        # Give a warning if the dataset is not in numpy format\n",
    "        if _has_tensor(dataset[0]):\n",
    "            warnings.warn(\"The dataset contains `torch.Tensor`. \"\n",
    "                \"Please make sure the dataset is in numpy format.\")\n",
    "        self._ds = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._ds[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TorchDataset` is a wrapper class of `torch.utils.data`. It does not modify inner behavior of the input pytorch `dataset`.\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "`TorchDataset` will **NOT** turn a `torch.Tensor` into `numpy.array`.\n",
    "Therefore, it is suggested to ensure the input `dataset` is in numpy format \n",
    "before passing to the `TorchDataset`.\n",
    "`TorchDataset` will give a warning if `torch.Tensor` is found in the dataset.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset \n",
    "using the Pytorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flatten and cast the PIL image into the `numpy.array`\n",
    "(brought from [jax official tutorial](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenAndCast(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.ravel(np.array(pic, dtype=float))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the pytorch [MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_torch = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can wrape the `mnist_torch` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ds = TorchDataset(mnist_torch)\n",
    "assert isinstance(mnist_ds[0][0], np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HFDataset(Dataset):\n",
    "    \"\"\"A Dataset class that wraps a huggingface Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: hf_datasets.Dataset # Huggingface Dataset\n",
    "    ):\n",
    "        _check_hf_installed()\n",
    "        # if not isinstance(dataset, hf_datasets.Dataset):\n",
    "        #     raise TypeError(f\"`dataset` must be a huggingface Dataset, \"\n",
    "        #                     f\"but got {type(dataset)}\")\n",
    "        # Ensure the dataset is in jax format\n",
    "        self._ds = dataset.with_format(\"jax\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._ds[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HFDataset` wraps a huggingface dataset. Unlike `TorchDataset`,\n",
    "`HFDataset` will ensure the input dataset with the format of `jax.DeviceArray`.\n",
    "\n",
    "Again, we load the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, \n",
    "but load the data via the huggingface datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset mnist (/home/birk/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "mnist_hf = load_dataset(\"mnist\", split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the `mnist_hf` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ds = HFDataset(mnist_hf)\n",
    "assert isinstance(mnist_ds[0]['image'], jnp.ndarray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseDataLoader:\n",
    "    \"\"\"Dataloader Interface\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset, \n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __next__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaderJax(BaseDataLoader):\n",
    "    \"\"\"Dataloder in Vanilla Jax\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Dataset,\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # drop last batches or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.keys = PRNGSequence(seed=get_config().global_seed) \\\n",
    "            if hk is None else hk.PRNGSequence(get_config().global_seed)\n",
    "        self.data_len = len(dataset)  # Length of the dataset\n",
    "        self.indices = jnp.arange(self.data_len) # available indices in the dataset\n",
    "        self.pose = 0  # record the current position in the dataset\n",
    "        self._shuffle()\n",
    "\n",
    "    def _shuffle(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = jax.random.permutation(next(self.keys), self.indices)\n",
    "        \n",
    "    def _stop_iteration(self):\n",
    "        self.pose = 0\n",
    "        self._shuffle()\n",
    "        raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            batches = len(self.dataset) // self.batch_size  # get the floor of division\n",
    "        else:\n",
    "            batches = -(len(self.dataset) // -self.batch_size)  # get the ceil of division\n",
    "        return batches\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pose + self.batch_size <= self.data_len:\n",
    "            batch_indices = self.indices[self.pose: self.pose + self.batch_size]\n",
    "            batch_data = self.dataset[batch_indices, ...]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        elif self.pose < self.data_len and not self.drop_last:\n",
    "            batch_indices = self.indices[self.pose:]\n",
    "            batch_data = self.dataset[batch_indices, ...]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        else:\n",
    "            self._stop_iteration()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_dataloader(dataloader_cls, samples=1000, batch_size=12):\n",
    "    feats = jnp.arange(samples).repeat(10).reshape(samples, 10)\n",
    "    labels = jnp.arange(samples).reshape(samples, 1)\n",
    "    ds = ArrayDataset(feats, labels)\n",
    "    # N % batchsize != 0\n",
    "    dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert jnp.array_equal(_X, feats)\n",
    "        assert jnp.array_equal(_Y, labels)\n",
    "\n",
    "    dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        last_idx = len(X_list) * batch_size\n",
    "        assert jnp.array_equal(_X, feats[: last_idx])\n",
    "        assert jnp.array_equal(_Y, labels[: last_idx])\n",
    "\n",
    "\n",
    "    dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl_shuffle:\n",
    "            assert jnp.array_equal(x[:, :1], y)\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert not jnp.array_equal(_X, feats)\n",
    "        assert not jnp.array_equal(_Y, labels)\n",
    "        assert jnp.sum(_X) == jnp.sum(feats), \\\n",
    "            f\"jnp.sum(_X)={jnp.sum(_X)}, jnp.sum(feats)={jnp.sum(feats)}\"\n",
    "\n",
    "\n",
    "    dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl_shuffle:\n",
    "            assert jnp.array_equal(x[:, :1], y)\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert not jnp.array_equal(_X, feats)\n",
    "        assert not jnp.array_equal(_Y, labels)\n",
    "        assert len(_X) == len(X_list) * batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_dataloader(DataLoaderJax, samples=20, batch_size=12)\n",
    "test_dataloader(DataLoaderJax, samples=20, batch_size=10)\n",
    "test_dataloader(DataLoaderJax, samples=11, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataloader\n",
    "\n",
    "Use `Pytorch` to load batches. It requires [pytorch](https://pytorch.org/get-started/) to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# copy from https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\n",
    "def _numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [_numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def _convert_dataset_pytorch(dataset: Dataset):\n",
    "    class DatasetPytorch(torch_data.Dataset):\n",
    "        def __init__(self, dataset: Dataset): self.dataset = dataset\n",
    "        def __len__(self): return len(self.dataset)\n",
    "        def __getitem__(self, idx): return self.dataset[idx]\n",
    "    \n",
    "    return DatasetPytorch(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaderPytorch(BaseDataLoader):\n",
    "    \"\"\"Pytorch Dataloader\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Dataset,\n",
    "        batch_size: int = 1,  # Batch size\n",
    "        shuffle: bool = False,  # If true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # Drop last batch or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(dataset, batch_size, shuffle, drop_last)\n",
    "        _check_pytorch_installed()\n",
    "        \n",
    "        dataset = _convert_dataset_pytorch(dataset)\n",
    "        self.dataloader = torch_data.DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            drop_last=drop_last,\n",
    "            collate_fn=_numpy_collate,\n",
    "            **kwargs\n",
    "        ) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_dataloader(DataLoaderPytorch, samples=20, batch_size=12)\n",
    "test_dataloader(DataLoaderPytorch, samples=20, batch_size=10)\n",
    "test_dataloader(DataLoaderPytorch, samples=11, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _is_hf_dataset(dataset):\n",
    "    return hf_datasets and (\n",
    "        isinstance(dataset, hf_datasets.Dataset) \n",
    "        or isinstance(dataset, hf_datasets.DatasetDict)\n",
    "    )\n",
    "\n",
    "def _dispatch_dataset(\n",
    "    dataset, # Dataset or Pytorch Dataset or HuggingFace Dataset\n",
    "):\n",
    "    if isinstance(dataset, Dataset):\n",
    "        return dataset\n",
    "    elif torch_data and isinstance(dataset, torch_data.Dataset):\n",
    "        return TorchDataset(dataset)\n",
    "    elif _is_hf_dataset(dataset):\n",
    "        return HFDataset(dataset)\n",
    "    else:\n",
    "        raise ValueError(f\"dataset must be one of `jax_loader.core.Dataset`, \"\n",
    "                         \"`torch.utils.data.Dataset`, `datasets.Dataset`, \"\n",
    "                         f\"but got {type(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(frozen=True)\n",
    "class DataloaderBackends:\n",
    "    jax: BaseDataLoader = DataLoaderJax\n",
    "    pytorch: BaseDataLoader = DataLoaderPytorch\n",
    "    tensorflow: BaseDataLoader = None\n",
    "    merlin: BaseDataLoader = None\n",
    "\n",
    "    __all__ = dict(\n",
    "        jax=jax, pytorch=pytorch, tensorflow=tensorflow, merlin=merlin\n",
    "    )\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.__all__[key]\n",
    "\n",
    "    @property\n",
    "    def supported(self) -> List[str]:\n",
    "        return [\n",
    "            backend for backend, dl_cls in self.__all__.items() if dl_cls is not None\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _dispatch_dataloader(\n",
    "    backend: str # dataloader backend\n",
    ") -> BaseDataLoader:\n",
    "    \"\"\"Return Dataloader class based on given `backend`\"\"\"\n",
    "    backends = DataloaderBackends()\n",
    "    if not backend in backends.supported:\n",
    "        raise ValueError(f\"backend=`{backend}` is either an invalid backend or not supported yet. \"\n",
    "            f\"Should be one of {backends.supported}.\")\n",
    "    \n",
    "    dl_cls = backends[backend]\n",
    "    return dl_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/jax-dataloader/blob/master/jax_dataloader/core.py#L330){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _dispatch_dataloader\n",
       "\n",
       ">      _dispatch_dataloader (backend:str)\n",
       "\n",
       "Return Dataloader class based on given `backend`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| backend | str | dataloader backend |\n",
       "| **Returns** | **BaseDataLoader** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/jax-dataloader/blob/master/jax_dataloader/core.py#L330){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _dispatch_dataloader\n",
       "\n",
       ">      _dispatch_dataloader (backend:str)\n",
       "\n",
       "Return Dataloader class based on given `backend`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| backend | str | dataloader backend |\n",
       "| **Returns** | **BaseDataLoader** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(_dispatch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoader:\n",
    "    \"\"\"Main Dataloader class to load Numpy data batches\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        backend: str, # Dataloader backend\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # drop last batches or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        dataset = _dispatch_dataset(dataset)\n",
    "        dataloader_cls = _dispatch_dataloader(backend)\n",
    "        self.dataloader = dataloader_cls(\n",
    "            dataset=dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            drop_last=drop_last,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader.__iter__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Minimum Example of using Dataloader\n",
    "\n",
    "We showcase how to use `Dataloader` for training a simple regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import optax\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=10000, n_features=20)\n",
    "dataset = ArrayDataset(X, y.reshape(-1, 1))\n",
    "keys = hk.PRNGSequence(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `loss`, `step`, `train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, x, y):\n",
    "    return jnp.mean(vmap(optax.l2_loss)(x @ w.T, y))\n",
    "\n",
    "def step(w, x, y):\n",
    "    lr = 0.1\n",
    "    grad = jax.grad(loss)(w, x, y)\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "def train(dataloader: DataLoader, key: jax.random.PRNGKey):\n",
    "    w = jax.random.normal(key, shape=(1, 20))\n",
    "    n_epochs = 10\n",
    "    for _ in range(n_epochs):\n",
    "        for x, y in dataloader:\n",
    "            w = step(w, x, y)\n",
    "    return w\n",
    "\n",
    "def eval(dataloader: DataLoader, w):\n",
    "    err = []\n",
    "    for x, y in dataloader:\n",
    "        err.append(loss(w, x, y))\n",
    "    return np.mean(err)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderJax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'jax', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, 'jax', batch_size=200, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderPytorch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'pytorch', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
