{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "> Support various dataloader for loading batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from typing import Callable, Union, List, Tuple, Dict, Any, Optional, TypeVar, Generic, Iterable, Sequence, Iterator\n",
    "import jax\n",
    "from jax import vmap, grad, jit, numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from abc import ABC\n",
    "from dataclasses import dataclass\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try: import torch.utils.data as torch_data\n",
    "except ModuleNotFoundError: torch_data = None\n",
    "\n",
    "try: import haiku as hk \n",
    "except ModuleNotFoundError: hk = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "#| hide\n",
    "@dataclass\n",
    "class Config:\n",
    "    rng_reserve_size: int\n",
    "    global_seed: int\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls) -> Config:\n",
    "        return cls(rng_reserve_size=1, global_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "main_config = Config.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def get_config() -> Config:\n",
    "    return main_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PRNGSequence(Iterator[PRNGKey]):\n",
    "    \"\"\"An Interator of Jax PRNGKey (minimal version of haiku.PRNGSequence).\"\"\"\n",
    "\n",
    "    def __init__(self, seed: int):\n",
    "        self._key = jax.random.PRNGKey(seed)\n",
    "        self._subkeys = collections.deque()\n",
    "\n",
    "    def reserve(self, num):\n",
    "        \"\"\"Splits additional ``num`` keys for later use.\"\"\"\n",
    "        if num > 0:\n",
    "            new_keys = tuple(jax.random.split(self._key, num + 1))\n",
    "            self._key = new_keys[0]\n",
    "            self._subkeys.extend(new_keys[1:])\n",
    "            \n",
    "    def __next__(self):\n",
    "        if not self._subkeys:\n",
    "            self.reserve(get_config().rng_reserve_size)\n",
    "        return self._subkeys.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset:\n",
    "    \"\"\"A simple pytorch-like Numpy Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, *arrays: jnp.DeviceArray):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return tuple(arr[idx] for arr in self.arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = PRNGSequence(seed=0)\n",
    "X = jax.random.normal(next(keys), shape=(1000, 10))\n",
    "y = jax.random.normal(next(keys), shape=(1000, ))\n",
    "ds = Dataset(X, y)\n",
    "\n",
    "assert len(ds) == 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing `Dataset` using `ds[idx]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = ds[1]\n",
    "assert jnp.array_equal(x1, X[1])\n",
    "assert jnp.array_equal(y1, y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseDataLoader(ABC):\n",
    "    \"\"\"Dataloader Interface\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset,\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        pass \n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __next__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaderJax(BaseDataLoader):\n",
    "    \"\"\"Dataloder in Vanilla Jax\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Dataset,\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # drop last batches or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.keys = PRNGSequence(seed=get_config().global_seed) \\\n",
    "            if hk is None else hk.PRNGSequence(get_config().global_seed)\n",
    "        self.data_len = len(dataset)  # Length of the dataset\n",
    "        self.indices = jnp.arange(self.data_len) # available indices in the dataset\n",
    "        self.pose = 0  # record the current position in the dataset\n",
    "        self._shuffle()\n",
    "\n",
    "    def _shuffle(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = jax.random.permutation(next(self.keys), self.indices)\n",
    "        \n",
    "    def _stop_iteration(self):\n",
    "        self.pose = 0\n",
    "        self._shuffle()\n",
    "        raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            batches = len(self.dataset) // self.batch_size  # get the floor of division\n",
    "        else:\n",
    "            batches = -(len(self.dataset) // -self.batch_size)  # get the ceil of division\n",
    "        return batches\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pose + self.batch_size <= self.data_len:\n",
    "            batch_indices = self.indices[self.pose: self.pose + self.batch_size]\n",
    "            batch_data = self.dataset[batch_indices, ...]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        elif self.pose < self.data_len and not self.drop_last:\n",
    "            batch_indices = self.indices[self.pose:]\n",
    "            batch_data = self.dataset[batch_indices, ...]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        else:\n",
    "            self._stop_iteration()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataloader(dataloader_cls, samples=1000, batch_size=12):\n",
    "    feats = jnp.arange(samples).repeat(10).reshape(samples, 10)\n",
    "    labels = jnp.arange(samples).reshape(samples, 1)\n",
    "    ds = Dataset(feats, labels)\n",
    "    # N % batchsize != 0\n",
    "    dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert jnp.array_equal(_X, feats)\n",
    "        assert jnp.array_equal(_Y, labels)\n",
    "\n",
    "    dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        last_idx = len(X_list) * batch_size\n",
    "        assert jnp.array_equal(_X, feats[: last_idx])\n",
    "        assert jnp.array_equal(_Y, labels[: last_idx])\n",
    "\n",
    "\n",
    "    dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl_shuffle:\n",
    "            assert jnp.array_equal(x[:, :1], y)\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert not jnp.array_equal(_X, feats)\n",
    "        assert not jnp.array_equal(_Y, labels)\n",
    "        assert jnp.sum(_X) == jnp.sum(feats), \\\n",
    "            f\"jnp.sum(_X)={jnp.sum(_X)}, jnp.sum(feats)={jnp.sum(feats)}\"\n",
    "\n",
    "\n",
    "    dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl_shuffle:\n",
    "            assert jnp.array_equal(x[:, :1], y)\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert not jnp.array_equal(_X, feats)\n",
    "        assert not jnp.array_equal(_Y, labels)\n",
    "        assert len(_X) == len(X_list) * batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_dataloader(DataLoaderJax, samples=1000, batch_size=12)\n",
    "test_dataloader(DataLoaderJax, samples=1000, batch_size=10)\n",
    "test_dataloader(DataLoaderJax, samples=1001, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataloader\n",
    "\n",
    "Use `Pytorch` to load batches. It requires [pytorch](https://pytorch.org/get-started/) to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# copy from https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\n",
    "def _numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [_numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def _convert_dataset_pytorch(dataset: Dataset):\n",
    "    class DatasetPytorch(torch_data.Dataset):\n",
    "        def __init__(self, dataset: Dataset): self.dataset = dataset\n",
    "        def __len__(self): return len(self.dataset)\n",
    "        def __getitem__(self, idx): return self.dataset[idx]\n",
    "    \n",
    "    return DatasetPytorch(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaderPytorch(BaseDataLoader):\n",
    "    \"\"\"Pytorch Dataloader\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Dataset,\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # drop last batch or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        if torch_data is None:\n",
    "            raise ModuleNotFoundError(\"`pytorch` library needs to be installed. Try `pip install torch`.\"\n",
    "            \"Please refer to pytorch documentation for details: https://pytorch.org/get-started/.\")\n",
    "        \n",
    "        dataset = _convert_dataset_pytorch(dataset)\n",
    "        self.dataloader = torch_data.DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            drop_last=drop_last,\n",
    "            collate_fn=_numpy_collate,\n",
    "            **kwargs\n",
    "        ) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_dataloader(DataLoaderPytorch, samples=1000, batch_size=12)\n",
    "test_dataloader(DataLoaderPytorch, samples=1000, batch_size=10)\n",
    "test_dataloader(DataLoaderPytorch, samples=1001, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(frozen=True)\n",
    "class DataloaderBackends:\n",
    "    jax: BaseDataLoader = DataLoaderJax\n",
    "    pytorch: BaseDataLoader = DataLoaderPytorch\n",
    "    tensorflow: BaseDataLoader = None\n",
    "    merlin: BaseDataLoader = None\n",
    "\n",
    "    __all__ = dict(\n",
    "        jax=jax, pytorch=pytorch, tensorflow=tensorflow, merlin=merlin\n",
    "    )\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.__all__[key]\n",
    "\n",
    "    @property\n",
    "    def supported(self) -> List[str]:\n",
    "        return [\n",
    "            backend for backend, dl_cls in self.__all__.items() if dl_cls is not None\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _dispatch_dataloader(\n",
    "    backend: str # dataloader backend\n",
    ") -> BaseDataLoader:\n",
    "    \"\"\"Return Dataloader class based on given `backend`\"\"\"\n",
    "    backends = DataloaderBackends()\n",
    "    if not backend in backends.supported:\n",
    "        raise ValueError(f\"backend=`{backend}` is either an invalid backend or not supported yet. \"\n",
    "            f\"Should be one of {backends.supported}.\")\n",
    "    \n",
    "    dl_cls = backends[backend]\n",
    "    return dl_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### _dispatch_dataloader\n",
       "\n",
       ">      _dispatch_dataloader (backend:str)\n",
       "\n",
       "Return Dataloader class based on given `backend`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| backend | str | dataloader backend |\n",
       "| **Returns** | **BaseDataLoader** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### _dispatch_dataloader\n",
       "\n",
       ">      _dispatch_dataloader (backend:str)\n",
       "\n",
       "Return Dataloader class based on given `backend`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| backend | str | dataloader backend |\n",
       "| **Returns** | **BaseDataLoader** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(_dispatch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoader:\n",
    "    \"\"\"Main Dataloader class to load Numpy data batches\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        backend: str, # Dataloader backend\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # drop last batches or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        dataloader_cls = _dispatch_dataloader(backend)\n",
    "        self.dataloader = dataloader_cls(\n",
    "            dataset=dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            drop_last=drop_last,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader.__iter__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Minimum Example of using Dataloader\n",
    "\n",
    "We showcase how to use `Dataloader` for training a simple regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import optax\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=10000, n_features=20)\n",
    "dataset = Dataset(X, y.reshape(-1, 1))\n",
    "keys = hk.PRNGSequence(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `loss`, `step`, `train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, x, y):\n",
    "    return jnp.mean(vmap(optax.l2_loss)(x @ w.T, y))\n",
    "\n",
    "def step(w, x, y):\n",
    "    lr = 0.1\n",
    "    grad = jax.grad(loss)(w, x, y)\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "def train(dataloader: DataLoader, key: jax.random.PRNGKey):\n",
    "    w = jax.random.normal(key, shape=(1, 20))\n",
    "    n_epochs = 10\n",
    "    for _ in range(n_epochs):\n",
    "        for x, y in dataloader:\n",
    "            w = step(w, x, y)\n",
    "    return w\n",
    "\n",
    "def eval(dataloader: DataLoader, w):\n",
    "    err = []\n",
    "    for x, y in dataloader:\n",
    "        err.append(loss(w, x, y))\n",
    "    return np.mean(err)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderJax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'jax', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, 'jax', batch_size=200, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderPytorch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'pytorch', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
