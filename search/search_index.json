{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dataloader for JAX","text":"<p>Overview | Installation | Usage | Documentation</p>"},{"location":"#overview","title":"Overview","text":"<p><code>jax_dataloader</code> brings pytorch-like dataloader API to <code>jax</code>. It supports</p> <ul> <li> <p>4 datasets to download and pre-process data:</p> </li> <li> <p>jax dataset</p> </li> <li>huggingface datasets</li> <li>pytorch     Dataset</li> <li> <p>tensorflow dataset</p> </li> <li> <p>3 backends to iteratively load batches:</p> </li> <li> <p>jax     dataloader</p> </li> <li>pytorch     dataloader</li> <li>tensorflow dataset</li> </ul> <p>A minimum <code>jax-dataloader</code> example:</p> <pre><code>import jax_dataloader as jdl\n\njdl.manual_seed(1234) # Set the global seed to 1234 for reproducibility\n\ndataloader = jdl.DataLoader(\n    dataset, # Can be a jdl.Dataset or pytorch or huggingface or tensorflow dataset\n    backend='jax', # Use 'jax' backend for loading data\n    batch_size=32, # Batch size \n    shuffle=True, # Shuffle the dataloader every iteration or not\n    drop_last=False, # Drop the last batch or not\n    generator=jdl.Generator() # Control the randomness of this dataloader \n)\n\nbatch = next(iter(dataloader)) # iterate next batch\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>The latest <code>jax-dataloader</code> release can directly be installed from PyPI:</p> <pre><code>pip install jax-dataloader\n</code></pre> <p>or install directly from the repository:</p> <pre><code>pip install git+https://github.com/BirkhoffG/jax-dataloader.git\n</code></pre> <p>Note</p> <p>We keep <code>jax-dataloader</code>'s dependencies minimum, which only install <code>jax</code> and <code>plum-dispatch</code> (for backend dispatching) when installing. If you wish to use integration of  <code>pytorch</code>,  huggingface <code>datasets</code>,  or <code>tensorflow</code>, we highly recommend manually install those dependencies.</p> <p>You can also run <code>pip install jax-dataloader[all]</code> to install everything (not recommended).</p>"},{"location":"#usage","title":"Usage","text":"<p><code>jax_dataloader.core.DataLoader</code> follows similar API as the pytorch dataloader.</p> <ul> <li>The <code>dataset</code> should be an object of the subclass of   <code>jax_dataloader.core.Dataset</code> or <code>torch.utils.data.Dataset</code> or (the   huggingface) <code>datasets.Dataset</code> or <code>tf.data.Dataset</code>.</li> <li>The <code>backend</code> should be one of <code>\"jax\"</code> or <code>\"pytorch\"</code> or   <code>\"tensorflow\"</code>. This argument specifies which backend dataloader to   load batches.</li> </ul> <p>Note that not every dataset is compatible with every backend. See the compatibility table below:</p> <code>jdl.Dataset</code> <code>torch_data.Dataset</code> <code>tf.data.Dataset</code> <code>datasets.Dataset</code> <code>\"jax\"</code> \u2705 \u274c \u274c \u2705 <code>\"pytorch\"</code> \u2705 \u2705 \u274c \u2705 <code>\"tensorflow\"</code> \u2705 \u274c \u2705 \u2705"},{"location":"#using-arraydataset","title":"Using <code>ArrayDataset</code>","text":"<p>The <code>jax_dataloader.core.ArrayDataset</code> is an easy way to wrap multiple <code>jax.numpy.array</code> into one Dataset. For example, we can create an <code>ArrayDataset</code> as follows:</p> <pre><code># Create features `X` and labels `y`\nX = jnp.arange(100).reshape(10, 10)\ny = jnp.arange(10)\n# Create an `ArrayDataset`\narr_ds = jdl.ArrayDataset(X, y)\n</code></pre> <p>This <code>arr_ds</code> can be loaded by every backends.</p> <pre><code># Create a `DataLoader` from the `ArrayDataset` via jax backend\ndataloader = jdl.DataLoader(arr_ds, 'jax', batch_size=5, shuffle=True)\n# Or we can use the pytorch backend\ndataloader = jdl.DataLoader(arr_ds, 'pytorch', batch_size=5, shuffle=True)\n# Or we can use the tensorflow backend\ndataloader = jdl.DataLoader(arr_ds, 'tensorflow', batch_size=5, shuffle=True)\n</code></pre>"},{"location":"#using-huggingface-datasets","title":"Using Huggingface Datasets","text":"<p>The huggingface datasets is a morden library for downloading, pre-processing, and sharing datasets. <code>jax_dataloader</code> supports directly passing the huggingface datasets.</p> <pre><code>from datasets import load_dataset\n</code></pre> <p>For example, We load the <code>\"squad\"</code> dataset from <code>datasets</code>:</p> <pre><code>hf_ds = load_dataset(\"squad\")\n</code></pre> <p>Then, we can use <code>jax_dataloader</code> to load batches of <code>hf_ds</code>.</p> <pre><code># Create a `DataLoader` from the `datasets.Dataset` via jax backend\ndataloader = jdl.DataLoader(hf_ds['train'], 'jax', batch_size=5, shuffle=True)\n# Or we can use the pytorch backend\ndataloader = jdl.DataLoader(hf_ds['train'], 'pytorch', batch_size=5, shuffle=True)\n# Or we can use the tensorflow backend\ndataloader = jdl.DataLoader(hf_ds['train'], 'tensorflow', batch_size=5, shuffle=True)\n</code></pre>"},{"location":"#using-pytorch-datasets","title":"Using Pytorch Datasets","text":"<p>The pytorch Dataset and its ecosystems (e.g., torchvision, torchtext, torchaudio) supports many built-in datasets. <code>jax_dataloader</code> supports directly passing the pytorch Dataset.</p> <p>Note</p> <p>Unfortuantely, the pytorch Dataset can only work with <code>backend=pytorch</code>. See the belowing example.</p> <pre><code>from torchvision.datasets import MNIST\nimport numpy as np\n</code></pre> <p>We load the MNIST dataset from <code>torchvision</code>. The <code>ToNumpy</code> object transforms images to <code>numpy.array</code>.</p> <pre><code>pt_ds = MNIST('/tmp/mnist/', download=True, transform=lambda x: np.array(x, dtype=float), train=False)\n</code></pre> <p>This <code>pt_ds</code> can only be loaded via <code>\"pytorch\"</code> dataloaders.</p> <pre><code>dataloader = jdl.DataLoader(pt_ds, 'pytorch', batch_size=5, shuffle=True)\n</code></pre>"},{"location":"#using-tensowflow-datasets","title":"Using Tensowflow Datasets","text":"<p><code>jax_dataloader</code> supports directly passing the tensorflow datasets.</p> <pre><code>import tensorflow_datasets as tfds\nimport tensorflow as tf\n</code></pre> <p>For instance, we can load the MNIST dataset from <code>tensorflow_datasets</code></p> <pre><code>tf_ds = tfds.load('mnist', split='test', as_supervised=True)\n</code></pre> <p>and use <code>jax_dataloader</code> for iterating the dataset.</p> <pre><code>dataloader = jdl.DataLoader(tf_ds, 'tensorflow', batch_size=5, shuffle=True)\n</code></pre>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Dataloader for JAX</li> <li>Controlling Randomness</li> <li>Examples<ul> <li>Vision Transformer</li> </ul> </li> <li>API<ul> <li>Core API</li> <li>Dataset</li> <li>Loaders<ul> <li>Dataloader</li> <li>JAX Dataloader</li> <li><code>Pytorch</code>-backed Dataloader Use <code>Pytorch</code> to load batches. It requires pytorch to be installed.</li> <li><code>Tensorflow</code>-backed Dataloader</li> </ul> </li> <li>Utils</li> </ul> </li> </ul>"},{"location":"changelog_not_found/","title":"Changelog not found","text":""},{"location":"changelog_not_found/#changelogmd-file-not-found","title":"CHANGELOG.md file not found.","text":"<p>To generate the changelog file, please run the following command from the project root directory. </p> <pre><code>nbdev_changelog\n</code></pre> <p>If you do not want this page to be rendered as part of the documentation, please remove the following line from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- [Releases]{changelog}\n</code></pre>"},{"location":"cli_commands_not_found/","title":"No CLI commands found in console_scripts in settings.ini file.","text":"<p>For documenting CLI commands, please add command line executables in <code>console_scripts</code> in <code>settings.ini</code> file. </p> <p>If you do not want this page to be rendered as part of the documentation, please remove the following lines from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- CLI\n{cli}\n</code></pre>"},{"location":"core/","title":"Core API","text":"<p>source</p>"},{"location":"core/#dataloaderbackends","title":"DataloaderBackends","text":"<p>DataloaderBackends                          (pytorch:jax_dataloader.loaders.base.BaseDataLoader=&lt;                          class                          'jax_dataloader.loaders.torch.DataLoaderPytorch'&gt;, te                          nsorflow:jax_dataloader.loaders.base.BaseDataLoader=&lt;                          class 'jax_dataloader.loaders.tensorflow.DataLoaderTe                          nsorflow'&gt;, merlin:jax_dataloader.loaders.base.BaseDa                          taLoader=None)</p> <p>source</p>"},{"location":"core/#get_backend_compatibilities","title":"get_backend_compatibilities","text":"<p>get_backend_compatibilities ()</p> <p>Return list of supported dataloader backends for each dataset type</p> <p>source</p>"},{"location":"core/#dataloader","title":"DataLoader","text":"<p>DataLoader (dataset, backend:Literal['jax','pytorch','tensorflow'],                  batch_size:int=1, shuffle:bool=False, drop_last:bool=False, g                  enerator:Union[jax_dataloader.utils.Generator,jax.Array,Forwa                  rdRef('torch.Generator'),NoneType]=None, **kwargs)</p> <p>Main Dataloader class to load Numpy data batches</p> Type Default Details dataset Dataset from which to load the data backend Literal[\u2018jax\u2019, \u2018pytorch\u2019, \u2018tensorflow\u2019] Dataloader backend to load the dataset batch_size int 1 How many samples per batch to load shuffle bool False If true, dataloader reshuffles every epoch drop_last bool False If true, drop the last incomplete batch generator Optional[GeneratorType] None Random seed generator kwargs"},{"location":"core/#a-minimum-example-of-using-dataloader","title":"A Minimum Example of using Dataloader","text":"<p>We showcase how to use <code>Dataloader</code> for training a simple regression model.</p> <pre><code>from sklearn.datasets import make_regression\nimport optax\nimport haiku as hk\n</code></pre> <pre><code>X, y = make_regression(n_samples=500, n_features=20)\ndataset = ArrayDataset(X, y.reshape(-1, 1))\nkeys = hk.PRNGSequence(0)\n</code></pre> <pre><code>An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</code></pre> <p>Define <code>loss</code>, <code>step</code>, <code>train</code>:</p> <pre><code>def loss(w, x, y):\n    return jnp.mean(vmap(optax.l2_loss)(x @ w.T, y))\n\ndef step(w, x, y):\n    lr = 0.1\n    grad = jax.grad(loss)(w, x, y)\n    w -= lr * grad\n    return w\n\ndef train(dataloader: DataLoader, key: jax.random.PRNGKey):\n    w = jrand.normal(key, shape=(1, 20))\n    n_epochs = 10\n    for _ in range(n_epochs):\n        for x, y in dataloader:\n            w = step(w, x, y)\n    return w\n\ndef eval(dataloader: DataLoader, w):\n    err = []\n    for x, y in dataloader:\n        err.append(loss(w, x, y))\n    return np.mean(err)\n</code></pre> <p>Train this linear regression model via <code>DataLoaderJAX</code>:</p> <pre><code>dataloader = DataLoader(\n    dataset, 'jax', batch_size=128, shuffle=True)\nw = train(dataloader, next(keys)).block_until_ready()\n# assert np.allclose(eval(dataloader, w), 0.)\n</code></pre> <pre><code>dataloader = DataLoader(dataset, 'jax', batch_size=200, shuffle=True)\nw = train(dataloader, next(keys)).block_until_ready()\n# assert np.allclose(eval(dataloader, w), 0.)\n</code></pre> <p>Train this linear regression model via <code>pytorch</code> backend:</p> <pre><code>dataloader = DataLoader(\n    dataset, 'pytorch', batch_size=128, shuffle=True)\nw = train(dataloader, next(keys)).block_until_ready()\n# assert np.allclose(eval(dataloader, w), 0.)\n</code></pre> <p>Train this linear regression model via <code>jax</code> backend:</p> <pre><code>dataloader = DataLoader(\n    dataset, 'tensorflow', batch_size=128, shuffle=True)\nw = train(dataloader, next(keys)).block_until_ready()\n# assert np.allclose(eval(dataloader, w), 0.)\n</code></pre>"},{"location":"dataset/","title":"Dataset","text":"<p>source</p>"},{"location":"dataset/#dataset_1","title":"Dataset","text":"<p>Dataset ()</p> <p>A pytorch-like Dataset class.</p> <p>source</p>"},{"location":"dataset/#arraydataset","title":"ArrayDataset","text":"<p>ArrayDataset (*arrays:jax.Array, asnumpy:bool=True)</p> <p>Dataset wrapping numpy arrays.</p> <p>This is similar to torch.utils.data.TensorDataset, but it wrapps numpy arrays.</p> <pre><code>X = jnp.arange(10000).reshape(1000, 10)\ny = jnp.arange(1000)\nds = ArrayDataset(X, y)\nassert len(ds) == 1000\n</code></pre> <p>We index numpy arrays along the first dimension. Dataset indexing is done via <code>ds[index]</code>.</p> <pre><code>x1, y1 = ds[1] # get the first sample\nassert jnp.array_equal(x1, X[1])\nassert jnp.array_equal(y1, y[1])\n\nx10, y10 = ds[:10]\nassert jnp.array_equal(x10, X[:10])\nassert jnp.array_equal(y10, y[:10])\n</code></pre> <p>By default, <code>ArrayDataset</code> stores arrays as numpy.array.</p> <pre><code>x, _ = ds[:10]\nassert isinstance(x, np.ndarray)\nassert not isinstance(x, jnp.ndarray)\n</code></pre> <p>If you want to store the array type the way you passed, you can simply pass <code>asnumpy=False</code>.</p> <pre><code>ds = ArrayDataset(X, y, asnumpy=False)\nx, _ = ds[:10]\nassert isinstance(x, jnp.ndarray)\n</code></pre>"},{"location":"loader.base/","title":"Dataloader","text":"<p>source</p>"},{"location":"loader.base/#basedataloader","title":"BaseDataLoader","text":"<p>BaseDataLoader (dataset, batch_size:int=1, shuffle:bool=False,                      num_workers:int=0, drop_last:bool=False, generator:Union[                      jax_dataloader.utils.Generator,jax.Array,ForwardRef('torc                      h.Generator'),NoneType]=None, **kwargs)</p> <p>Dataloader Interface</p> Type Default Details dataset batch_size int 1 batch size shuffle bool False if true, dataloader shuffles before sampling each batch num_workers int 0 how many subprocesses to use for data loading. drop_last bool False generator Optional[GeneratorType] None kwargs"},{"location":"loader.jax/","title":"JAX Dataloader","text":"<p>source</p>"},{"location":"loader.jax/#epochiterator","title":"EpochIterator","text":"<p>EpochIterator (data, batch_size:int, indices:Sequence[int])</p> <p>source</p>"},{"location":"loader.jax/#to_jax_dataset","title":"to_jax_dataset","text":"<p>to_jax_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.jax/#to_jax_dataset_1","title":"to_jax_dataset","text":"<p>to_jax_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.jax/#dataloaderjax","title":"DataLoaderJAX","text":"<p>DataLoaderJAX (dataset:Union[jax_dataloader.datasets.Dataset,Annotated[Un                     ion[datasets.arrow_dataset.Dataset,datasets.dataset_dict.D                     atasetDict,datasets.dataset_dict.IterableDatasetDict,datas                     ets.iterable_dataset.IterableDataset],beartype.vale.Is[lam                     bda_:hf_datasetsisnotNone]]], batch_size:int=1,                     shuffle:bool=False, num_workers:int=0,                     drop_last:bool=False, generator:Union[jax_dataloader.utils                     .Generator,jax.Array,ForwardRef('torch.Generator'),NoneTyp                     e]=None, **kwargs)</p> <p>Dataloader Interface</p> Type Default Details dataset Union batch_size int 1 batch size shuffle bool False if true, dataloader shuffles before sampling each batch num_workers int 0 how many subprocesses to use for data loading. Ignored. drop_last bool False if true, drop the last incomplete batch generator Union None random seed generator kwargs <pre><code>samples = 1280\nbatch_size = 12\nfeats = np.arange(samples).repeat(10).reshape(samples, 10)\nlabels = np.arange(samples).reshape(samples, 1)\nds = ArrayDataset(feats, labels)\ndl = DataLoaderJAX(ds, batch_size=batch_size, shuffle=True)\nassert len(dl) == 1280 // 12 + 1\nassert len(dl.indices) == 1280\n</code></pre> <pre><code>samples = 128\nbatch_size = 128\nfeats = np.arange(samples).repeat(10).reshape(samples, 10)\nlabels = np.arange(samples).reshape(samples, 1)\nds = ArrayDataset(feats, labels)\ndl = DataLoaderJAX(ds, batch_size=batch_size, shuffle=True, drop_last=True)\nassert len(dl) == 1\ndl = DataLoaderJAX(ds, batch_size=batch_size, shuffle=True, drop_last=False)\nassert len(dl) == 1\n</code></pre> <pre><code>test_dataloader(DataLoaderJAX, samples=1280, batch_size=10)\n</code></pre> <pre><code>281 ms \u00b1 27.8 ms per loop (mean \u00b1 std. dev. of 3 runs, 5 loops each)\n</code></pre>"},{"location":"loader.tf/","title":"Tensorflow-backed Dataloader","text":""},{"location":"loader.tf/#tensorflow-backed-dataloader","title":"<code>Tensorflow</code>-backed Dataloader","text":"<p>source</p>"},{"location":"loader.tf/#to_tf_dataset","title":"to_tf_dataset","text":"<p>to_tf_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.tf/#to_tf_dataset_1","title":"to_tf_dataset","text":"<p>to_tf_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.tf/#to_tf_dataset_2","title":"to_tf_dataset","text":"<p>to_tf_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.tf/#dataloadertensorflow","title":"DataLoaderTensorflow","text":"<p>DataLoaderTensorflow (dataset:Union[jax_dataloader.datasets.Dataset,tenso                            rflow.python.data.ops.dataset_ops.DatasetV2,Annotat                            ed[Union[datasets.arrow_dataset.Dataset,datasets.da                            taset_dict.DatasetDict,datasets.dataset_dict.Iterab                            leDatasetDict,datasets.iterable_dataset.IterableDat                            aset],beartype.vale.Is[lambda_:hf_datasetsisnotNone                            ]]], batch_size:int=1, shuffle:bool=False,                            drop_last:bool=False, generator:Union[jax_dataloade                            r.utils.Generator,jax.Array,ForwardRef('torch.Gener                            ator'),NoneType]=None, **kwargs)</p> <p>Tensorflow Dataloader</p> Type Default Details dataset Union batch_size int 1 Batch size shuffle bool False If true, dataloader shuffles before sampling each batch drop_last bool False Drop last batch or not generator Union None Random seed generator kwargs <p>source</p>"},{"location":"loader.tf/#get_seed","title":"get_seed","text":"<p>get_seed (generator:Union[jax_dataloader.utils.Generator,jax.Array,torch.                _C.Generator,NoneType]=None)</p> <pre><code>test_dataloader(DataLoaderTensorflow, samples=20, batch_size=12)\ntest_dataloader(DataLoaderTensorflow, samples=20, batch_size=10)\ntest_dataloader(DataLoaderTensorflow, samples=11, batch_size=10)\n</code></pre> <pre><code>An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</code></pre> <pre><code>test_dataloader(DataLoaderTensorflow, ds_type='tf', samples=40, batch_size=11)\ntest_dataloader(DataLoaderTensorflow, ds_type='hf', samples=40, batch_size=11)\n</code></pre>"},{"location":"loader.torch/","title":"Pytorch-backed Dataloader\nUse Pytorch to load batches. It requires pytorch to be installed.","text":""},{"location":"loader.torch/#pytorch-backed-dataloader","title":"<code>Pytorch</code>-backed Dataloader","text":"<p>Use <code>Pytorch</code> to load batches. It requires pytorch to be installed.</p> <p>source</p>"},{"location":"loader.torch/#to_torch_dataset","title":"to_torch_dataset","text":"<p>to_torch_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.torch/#to_torch_dataset_1","title":"to_torch_dataset","text":"<p>to_torch_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.torch/#to_torch_dataset_2","title":"to_torch_dataset","text":"<p>to_torch_dataset (dataset:jax_dataloader.datasets.Dataset)</p> <p>source</p>"},{"location":"loader.torch/#dataloaderpytorch","title":"DataLoaderPytorch","text":"<p>DataLoaderPytorch (dataset:Union[jax_dataloader.datasets.Dataset,torch.ut                         ils.data.dataset.Dataset,Annotated[Union[datasets.arro                         w_dataset.Dataset,datasets.dataset_dict.DatasetDict,da                         tasets.dataset_dict.IterableDatasetDict,datasets.itera                         ble_dataset.IterableDataset],beartype.vale.Is[lambda_:                         hf_datasetsisnotNone]]], batch_size:int=1,                         shuffle:bool=False, drop_last:bool=False, generator:Un                         ion[jax_dataloader.utils.Generator,jax.Array,ForwardRe                         f('torch.Generator'),NoneType]=None, **kwargs)</p> <p>Pytorch Dataloader</p> Type Default Details dataset Union batch_size int 1 Batch size shuffle bool False If true, dataloader shuffles before sampling each batch drop_last bool False Drop last batch or not generator Union None kwargs <pre><code>samples = 1280\nbatch_size = 12\nfeats = np.arange(samples).repeat(10).reshape(samples, 10)\nlabels = np.arange(samples).reshape(samples, 1)\n\nds_torch = torch_data.TensorDataset(torch.from_numpy(feats), torch.from_numpy(labels))\nds_array = ArrayDataset(feats, labels)\n</code></pre> <pre><code>dl_1 = DataLoaderPytorch(ds_torch, batch_size=batch_size, shuffle=True)\n\nfor _ in range(10):\n    for (x, y) in dl_1: \n        assert isinstance(x, np.ndarray)\n\ndl_2 = DataLoaderPytorch(ds_array, batch_size=batch_size, shuffle=True)\nfor (x, y) in dl_2: \n    assert isinstance(x, np.ndarray)\n</code></pre>"},{"location":"tests/","title":"Tests","text":"<p>source</p>"},{"location":"tests/#test_shuffle_reproducible","title":"test_shuffle_reproducible","text":"<p>test_shuffle_reproducible (cls, ds, batch_size:int, feats, labels)</p> <p>Test that the shuffle is reproducible</p> <p>source</p>"},{"location":"tests/#test_dataloader","title":"test_dataloader","text":"<p>test_dataloader (cls, ds_type='jax', samples=1000, batch_size=12)</p> <pre><code>from jax_dataloader.loaders import DataLoaderJAX\n</code></pre> <pre><code>test_dataloader(DataLoaderJAX, ds_type='jax')\n</code></pre> <pre><code># def test_dataloader(dataloader_cls, samples=1000, batch_size=12):\n#     feats = jnp.arange(samples).repeat(10).reshape(samples, 10)\n#     labels = jnp.arange(samples).reshape(samples, 1)\n#     ds = ArrayDataset(feats, labels)\n#     # N % batchsize != 0\n#     dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False)\n#     for _ in range(2):\n#         X_list, Y_list = [], []\n#         for x, y in dl:\n#             X_list.append(x)\n#             Y_list.append(y)\n#         _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n#         assert jnp.array_equal(_X, feats)\n#         assert jnp.array_equal(_Y, labels)\n\n#     dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False, drop_last=True)\n#     for _ in range(2):\n#         X_list, Y_list = [], []\n#         for x, y in dl:\n#             X_list.append(x)\n#             Y_list.append(y)\n#         _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n#         last_idx = len(X_list) * batch_size\n#         assert jnp.array_equal(_X, feats[: last_idx])\n#         assert jnp.array_equal(_Y, labels[: last_idx])\n\n\n#     dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n#     last_X, last_Y = jnp.array([]), jnp.array([])\n#     for _ in range(2):\n#         X_list, Y_list = [], []\n#         for x, y in dl_shuffle:\n#             assert jnp.array_equal(x[:, :1], y)\n#             X_list.append(x)\n#             Y_list.append(y)\n#         _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n#         assert not jnp.array_equal(_X, feats)\n#         assert not jnp.array_equal(_Y, labels)\n#         assert jnp.sum(_X) == jnp.sum(feats), \\\n#             f\"jnp.sum(_X)={jnp.sum(_X)}, jnp.sum(feats)={jnp.sum(feats)}\"\n#         assert not jnp.array_equal(_X, last_X)\n#         assert not jnp.array_equal(_Y, last_Y)\n#         last_X, last_Y = _X, _Y\n\n\n#     dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n#     for _ in range(2):\n#         X_list, Y_list = [], []\n#         for x, y in dl_shuffle:\n#             assert jnp.array_equal(x[:, :1], y)\n#             X_list.append(x)\n#             Y_list.append(y)\n#         _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n#         assert not jnp.array_equal(_X, feats)\n#         assert not jnp.array_equal(_Y, labels)\n#         assert len(_X) == len(X_list) * batch_size\n</code></pre>"},{"location":"utils/","title":"Utils","text":"<p>Global configs, PRNGSequence, check installed.</p>"},{"location":"utils/#configs","title":"Configs","text":"<p>source</p>"},{"location":"utils/#config","title":"Config","text":"<p>Config (rng_reserve_size:int, global_seed:int)</p> <p>Global configuration for the library</p> <p>source</p>"},{"location":"utils/#get_config","title":"get_config","text":"<p>get_config ()</p> <p>source</p>"},{"location":"utils/#manual_seed","title":"manual_seed","text":"<p>manual_seed (seed:int)</p> <p>Set the seed for the library</p> <pre><code>manual_seed(11)\nassert get_config().global_seed == 11\n</code></pre>"},{"location":"utils/#check-installation","title":"Check Installation","text":"<p>source</p>"},{"location":"utils/#check_pytorch_installed","title":"check_pytorch_installed","text":"<p>check_pytorch_installed ()</p> <pre><code>check_pytorch_installed()\n</code></pre> <p>source</p>"},{"location":"utils/#has_pytorch_tensor","title":"has_pytorch_tensor","text":"<p>has_pytorch_tensor (batch)</p> <p>source</p>"},{"location":"utils/#check_hf_installed","title":"check_hf_installed","text":"<p>check_hf_installed ()</p> <pre><code>check_hf_installed()\n</code></pre> <p>source</p>"},{"location":"utils/#check_tf_installed","title":"check_tf_installed","text":"<p>check_tf_installed ()</p> <pre><code>check_tf_installed()\n</code></pre>"},{"location":"utils/#seed-generator","title":"Seed Generator","text":"<p>source</p>"},{"location":"utils/#generator","title":"Generator","text":"<p>Generator (generator:jax.Array|torch._C.Generator=None)</p> <p>A wrapper around JAX and PyTorch generators. This is used to generate random numbers in a reproducible way.</p> <pre><code># Example of using the generator\ng = Generator()\nassert g.seed() == get_config().global_seed\nassert jnp.array_equal(g.jax_generator(), jax.random.PRNGKey(get_config().global_seed)) \nassert g.torch_generator().initial_seed() == get_config().global_seed\n\n# Examples of using the generator when passing a `jax.random.PRNGKey` or `torch.Generator`\ng_jax = Generator(generator=jax.random.PRNGKey(123))\nassert jnp.array_equal(g_jax.jax_generator(), jax.random.PRNGKey(123))\nassert g_jax.seed() is None\n\ng_torch = Generator(generator=torch.Generator().manual_seed(123))\nassert g_torch.torch_generator().initial_seed() == 123\nassert g_torch.seed() == 123\nassert jnp.array_equal(g_torch.jax_generator(), jax.random.PRNGKey(123))\n</code></pre> <pre><code># Example of using `manual_seed` to set the seed\ng_jax.manual_seed(456)\nassert g_jax.seed() == 456\nassert jnp.array_equal(g_jax.jax_generator(), jax.random.PRNGKey(456))\nassert g_jax.torch_generator().initial_seed() == 456\n\ng_torch.manual_seed(789)\nassert g_torch.seed() == 789\nassert g_torch.torch_generator().initial_seed() == 789\nassert jnp.array_equal(g_torch.jax_generator(), jax.random.PRNGKey(789))\n</code></pre>"},{"location":"utils/#util-functions","title":"Util Functions","text":"<p>source</p>"},{"location":"utils/#asnumpy","title":"asnumpy","text":"<p>asnumpy (x)</p> <pre><code>np_x = np.array([1, 2, 3])\njnp_x = jnp.array([1, 2, 3])\ntorch_x = torch.tensor([1, 2, 3])\ntf_x = tf.constant([1, 2, 3])\nassert np.array_equal(asnumpy(np_x), np_x)\nassert np.array_equal(asnumpy(jnp_x), np_x) and not isinstance(asnumpy(jnp_x), jnp.ndarray)\nassert np.array_equal(asnumpy(torch_x), np_x) and not isinstance(asnumpy(torch_x), torch.Tensor)\nassert np.array_equal(asnumpy(tf_x), np_x) and not isinstance(asnumpy(tf_x), tf.Tensor)\n</code></pre>"},{"location":"api/jax_dataloader/core/DataLoader/","title":"DataLoader","text":""},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader","title":"<code>jax_dataloader.core.DataLoader</code>","text":"<p>Main Dataloader class to load Numpy data batches</p> Source code in <code>jax_dataloader/core.py</code> <pre><code>class DataLoader:\n    \"\"\"Main Dataloader class to load Numpy data batches\"\"\"\n\n    def __init__(\n        self,\n        dataset, # Dataset from which to load the data\n        backend: Literal['jax', 'pytorch', 'tensorflow'], # Dataloader backend to load the dataset\n        batch_size: int = 1,  # How many samples per batch to load\n        shuffle: bool = False,  # If true, dataloader reshuffles every epoch\n        drop_last: bool = False, # If true, drop the last incomplete batch\n        generator: Optional[GeneratorType] = None, # Random seed generator\n        **kwargs\n    ):\n        dl_cls = _dispatch_dataloader(backend)\n        self.dataloader = dl_cls(\n            dataset=dataset, \n            batch_size=batch_size, \n            shuffle=shuffle, \n            drop_last=drop_last,\n            generator=generator,\n            **kwargs\n        )\n\n    def __len__(self):\n        return len(self.dataloader)\n\n    def __next__(self):\n        return next(self.dataloader)\n\n    def __iter__(self):\n        return iter(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader.dataloader","title":"<code>dataloader = dl_cls(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, generator=generator, **kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader.__init__","title":"<code>__init__(dataset, backend: Literal['jax', 'pytorch', 'tensorflow'], batch_size: int = 1, shuffle: bool = False, drop_last: bool = False, generator: Optional[GeneratorType] = None, **kwargs)</code>","text":"Source code in <code>jax_dataloader/core.py</code> <pre><code>def __init__(\n    self,\n    dataset, # Dataset from which to load the data\n    backend: Literal['jax', 'pytorch', 'tensorflow'], # Dataloader backend to load the dataset\n    batch_size: int = 1,  # How many samples per batch to load\n    shuffle: bool = False,  # If true, dataloader reshuffles every epoch\n    drop_last: bool = False, # If true, drop the last incomplete batch\n    generator: Optional[GeneratorType] = None, # Random seed generator\n    **kwargs\n):\n    dl_cls = _dispatch_dataloader(backend)\n    self.dataloader = dl_cls(\n        dataset=dataset, \n        batch_size=batch_size, \n        shuffle=shuffle, \n        drop_last=drop_last,\n        generator=generator,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader.__iter__","title":"<code>__iter__()</code>","text":"Source code in <code>jax_dataloader/core.py</code> <pre><code>def __iter__(self):\n    return iter(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader.__len__","title":"<code>__len__()</code>","text":"Source code in <code>jax_dataloader/core.py</code> <pre><code>def __len__(self):\n    return len(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/core/DataLoader/#jax_dataloader.core.DataLoader.__next__","title":"<code>__next__()</code>","text":"Source code in <code>jax_dataloader/core.py</code> <pre><code>def __next__(self):\n    return next(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/core/DataloaderBackends/","title":"DataloaderBackends","text":"<pre><code>options:\n  filters: []\n\n\noptions:\n  filters: []\n</code></pre>"},{"location":"api/jax_dataloader/core/DataloaderBackends/#jax_dataloader.core.DataloaderBackends","title":"<code>jax_dataloader.core.DataloaderBackends</code>","text":"Source code in <code>jax_dataloader/core.py</code> <pre><code>@dataclass(frozen=True)\nclass DataloaderBackends:\n    jax = DataLoaderJAX\n    pytorch: BaseDataLoader = DataLoaderPytorch\n    tensorflow: BaseDataLoader = DataLoaderTensorflow\n    merlin: BaseDataLoader = None\n\n    __all__ = dict(\n        jax=jax, pytorch=pytorch, tensorflow=tensorflow, merlin=merlin\n    )\n\n    def __getitem__(self, key):\n        return self.__all__[key]\n\n    @property\n    def supported(self) -&gt; List[str]:\n        return [\n            backend for backend, dl_cls in self.__all__.items() if dl_cls is not None\n        ]\n</code></pre>"},{"location":"api/jax_dataloader/core/DataloaderBackends/#jax_dataloader.core.DataloaderBackends-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/core/DataloaderBackends/#jax_dataloader.core.DataloaderBackends.__all__","title":"<code>__all__ = dict(jax=jax, pytorch=pytorch, tensorflow=tensorflow, merlin=merlin)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/core/DataloaderBackends/#jax_dataloader.core.DataloaderBackends.merlin","title":"<code>merlin: BaseDataLoader = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/core/DataloaderBackends/#jax_dataloader.core.DataloaderBackends.supported","title":"<code>supported: List[str]</code>  <code>property</code>","text":""},{"location":"api/jax_dataloader/core/DataloaderBackends/#jax_dataloader.core.DataloaderBackends-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/core/DataloaderBackends/#jax_dataloader.core.DataloaderBackends.__getitem__","title":"<code>__getitem__(key)</code>","text":"Source code in <code>jax_dataloader/core.py</code> <pre><code>def __getitem__(self, key):\n    return self.__all__[key]\n</code></pre>"},{"location":"api/jax_dataloader/core/get_backend_compatibilities/","title":"Get backend compatibilities","text":""},{"location":"api/jax_dataloader/core/get_backend_compatibilities/#jax_dataloader.core.get_backend_compatibilities","title":"<code>jax_dataloader.core.get_backend_compatibilities() -&gt; dict[str, list[type]]</code>","text":"<p>Return list of supported dataloader backends for each dataset type</p> Source code in <code>jax_dataloader/core.py</code> <pre><code>def get_backend_compatibilities() -&gt; dict[str, list[type]]: # { backend: [supported datasets] }\n    \"\"\"Return list of supported dataloader backends for each dataset type\"\"\"\n\n    ds = {\n        JAXDataset: ArrayDataset(np.array([1,2,3])),\n        TorchDataset: torch_data.Dataset(),\n        TFDataset: tf.data.Dataset.from_tensor_slices(np.array([1,2,3])),\n        HFDataset: hf_datasets.Dataset.from_dict({'a': [1,2,3]})\n    }\n    assert len(ds) == len(SUPPORTED_DATASETS)\n    backends = {b: [] for b in _get_backends()}\n    for b in _get_backends():\n        for name, dataset in ds.items():\n            try:\n                _check_backend_compatibility(dataset, b)\n                backends[b].append(name)\n            except:\n                pass\n\n    return backends\n</code></pre>"},{"location":"api/jax_dataloader/datasets/ArrayDataset/","title":"ArrayDataset","text":""},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset","title":"<code>jax_dataloader.datasets.ArrayDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset wrapping numpy arrays.</p> Source code in <code>jax_dataloader/datasets.py</code> <pre><code>class ArrayDataset(Dataset):\n    \"\"\"Dataset wrapping numpy arrays.\"\"\"\n\n    def __init__(\n        self, \n        *arrays: jax.Array, # Numpy array with same first dimension\n        asnumpy: bool = True, # Store arrays as numpy arrays if True; otherwise store as array type of *arrays\n    ):\n        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n            \"All arrays must have the same dimension.\"\n        self.arrays = tuple(arrays)\n        if asnumpy:\n            self.asnumpy()            \n\n    def asnumpy(self):\n        \"\"\"Convert all arrays to numpy arrays.\"\"\"\n        self.arrays = tuple(asnumpy(arr) for arr in self.arrays)\n\n    def __len__(self):\n        return self.arrays[0].shape[0]\n\n    def __getitem__(self, index):\n        return jax.tree_util.tree_map(lambda x: x[index], self.arrays)\n</code></pre>"},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset.arrays","title":"<code>arrays = tuple(arrays)</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"Source code in <code>jax_dataloader/datasets.py</code> <pre><code>def __getitem__(self, index):\n    return jax.tree_util.tree_map(lambda x: x[index], self.arrays)\n</code></pre>"},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset.__init__","title":"<code>__init__(*arrays: jax.Array, asnumpy: bool = True)</code>","text":"Source code in <code>jax_dataloader/datasets.py</code> <pre><code>def __init__(\n    self, \n    *arrays: jax.Array, # Numpy array with same first dimension\n    asnumpy: bool = True, # Store arrays as numpy arrays if True; otherwise store as array type of *arrays\n):\n    assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n        \"All arrays must have the same dimension.\"\n    self.arrays = tuple(arrays)\n    if asnumpy:\n        self.asnumpy()            \n</code></pre>"},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset.__len__","title":"<code>__len__()</code>","text":"Source code in <code>jax_dataloader/datasets.py</code> <pre><code>def __len__(self):\n    return self.arrays[0].shape[0]\n</code></pre>"},{"location":"api/jax_dataloader/datasets/ArrayDataset/#jax_dataloader.datasets.ArrayDataset.asnumpy","title":"<code>asnumpy()</code>","text":"<p>Convert all arrays to numpy arrays.</p> Source code in <code>jax_dataloader/datasets.py</code> <pre><code>def asnumpy(self):\n    \"\"\"Convert all arrays to numpy arrays.\"\"\"\n    self.arrays = tuple(asnumpy(arr) for arr in self.arrays)\n</code></pre>"},{"location":"api/jax_dataloader/datasets/Dataset/","title":"Dataset","text":""},{"location":"api/jax_dataloader/datasets/Dataset/#jax_dataloader.datasets.Dataset","title":"<code>jax_dataloader.datasets.Dataset</code>","text":"<p>A pytorch-like Dataset class.</p> Source code in <code>jax_dataloader/datasets.py</code> <pre><code>class Dataset:\n    \"\"\"A pytorch-like Dataset class.\"\"\"\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n</code></pre>"},{"location":"api/jax_dataloader/datasets/Dataset/#jax_dataloader.datasets.Dataset-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/datasets/Dataset/#jax_dataloader.datasets.Dataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"Source code in <code>jax_dataloader/datasets.py</code> <pre><code>def __getitem__(self, index):\n    raise NotImplementedError\n</code></pre>"},{"location":"api/jax_dataloader/datasets/Dataset/#jax_dataloader.datasets.Dataset.__len__","title":"<code>__len__()</code>","text":"Source code in <code>jax_dataloader/datasets.py</code> <pre><code>def __len__(self):\n    raise NotImplementedError\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/","title":"EpochIterator","text":""},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator","title":"<code>jax_dataloader.experimental.multi_processing.EpochIterator</code>","text":"<p>               Bases: <code>Thread</code></p> <p>[WIP] Multiprocessing Epoch Iterator</p> Source code in <code>jax_dataloader/experimental/multi_processing.py</code> <pre><code>class EpochIterator(Thread):\n    \"\"\"[WIP] Multiprocessing Epoch Iterator\"\"\"\n\n    def __init__(self, data, batch_size: int, indices: Sequence[int]):\n        super().__init__()\n        self.data = data\n        batches = chunk(indices, batch_size)\n        self.iter_idx = iter(batches)\n        self.output_queue = Queue(5) # TODO: maxsize\n        self.terminate_event = Event()\n        self.start()\n\n    def run(self):\n        try:\n            while True:\n                # get data\n                result = self.get_data()\n                # put result in queue\n                while True:\n                    try: \n                        self.output_queue.put(result, block=True, timeout=0.5)\n                        break\n                    except Full: pass\n\n                    if self.terminate_event.is_set(): return                \n\n        except StopIteration:\n            self.output_queue.put(None)\n\n    def __next__(self):\n        result = self.output_queue.get()\n        if result is None:\n            self.close()\n            raise StopIteration()\n        return result\n\n    def __iter__(self):\n        return self\n\n    def __del__(self):\n        self.close()\n\n    def close(self):\n        self.terminate_event.set()\n\n    def get_data(self):\n        batch_idx = next(self.iter_idx)\n        batch = self.data[batch_idx]\n        return batch\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.data","title":"<code>data = data</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.iter_idx","title":"<code>iter_idx = iter(batches)</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.output_queue","title":"<code>output_queue = Queue(5)</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.terminate_event","title":"<code>terminate_event = Event()</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.__del__","title":"<code>__del__()</code>","text":"Source code in <code>jax_dataloader/experimental/multi_processing.py</code> <pre><code>def __del__(self):\n    self.close()\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.__init__","title":"<code>__init__(data, batch_size: int, indices: Sequence[int])</code>","text":"Source code in <code>jax_dataloader/experimental/multi_processing.py</code> <pre><code>def __init__(self, data, batch_size: int, indices: Sequence[int]):\n    super().__init__()\n    self.data = data\n    batches = chunk(indices, batch_size)\n    self.iter_idx = iter(batches)\n    self.output_queue = Queue(5) # TODO: maxsize\n    self.terminate_event = Event()\n    self.start()\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.__iter__","title":"<code>__iter__()</code>","text":"Source code in <code>jax_dataloader/experimental/multi_processing.py</code> <pre><code>def __iter__(self):\n    return self\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#jax_dataloader.experimental.multi_processing.EpochIterator.__next__","title":"<code>__next__()</code>","text":"Source code in <code>jax_dataloader/experimental/multi_processing.py</code> <pre><code>def __next__(self):\n    result = self.output_queue.get()\n    if result is None:\n        self.close()\n        raise StopIteration()\n    return result\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#threading.Thread.getName","title":"<code>getName()</code>","text":"<p>Return a string used for identification purposes only.</p> <p>This method is deprecated, use the name attribute instead.</p> Source code in <code>python3.10/threading.py</code> <pre><code>def getName(self):\n    \"\"\"Return a string used for identification purposes only.\n\n    This method is deprecated, use the name attribute instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('getName() is deprecated, get the name attribute instead',\n                  DeprecationWarning, stacklevel=2)\n    return self.name\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#threading.Thread.isDaemon","title":"<code>isDaemon()</code>","text":"<p>Return whether this thread is a daemon.</p> <p>This method is deprecated, use the daemon attribute instead.</p> Source code in <code>python3.10/threading.py</code> <pre><code>def isDaemon(self):\n    \"\"\"Return whether this thread is a daemon.\n\n    This method is deprecated, use the daemon attribute instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                  DeprecationWarning, stacklevel=2)\n    return self.daemon\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#threading.Thread.is_alive","title":"<code>is_alive()</code>","text":"<p>Return whether the thread is alive.</p> <p>This method returns True just before the run() method starts until just after the run() method terminates. See also the module function enumerate().</p> Source code in <code>python3.10/threading.py</code> <pre><code>def is_alive(self):\n    \"\"\"Return whether the thread is alive.\n\n    This method returns True just before the run() method starts until just\n    after the run() method terminates. See also the module function\n    enumerate().\n\n    \"\"\"\n    assert self._initialized, \"Thread.__init__() not called\"\n    if self._is_stopped or not self._started.is_set():\n        return False\n    self._wait_for_tstate_lock(False)\n    return not self._is_stopped\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#threading.Thread.join","title":"<code>join(timeout=None)</code>","text":"<p>Wait until the thread terminates.</p> <p>This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs.</p> <p>When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call is_alive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out.</p> <p>When the timeout argument is not present or None, the operation will block until the thread terminates.</p> <p>A thread can be join()ed many times.</p> <p>join() raises a RuntimeError if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to join() a thread before it has been started and attempts to do so raises the same exception.</p> Source code in <code>python3.10/threading.py</code> <pre><code>def join(self, timeout=None):\n    \"\"\"Wait until the thread terminates.\n\n    This blocks the calling thread until the thread whose join() method is\n    called terminates -- either normally or through an unhandled exception\n    or until the optional timeout occurs.\n\n    When the timeout argument is present and not None, it should be a\n    floating point number specifying a timeout for the operation in seconds\n    (or fractions thereof). As join() always returns None, you must call\n    is_alive() after join() to decide whether a timeout happened -- if the\n    thread is still alive, the join() call timed out.\n\n    When the timeout argument is not present or None, the operation will\n    block until the thread terminates.\n\n    A thread can be join()ed many times.\n\n    join() raises a RuntimeError if an attempt is made to join the current\n    thread as that would cause a deadlock. It is also an error to join() a\n    thread before it has been started and attempts to do so raises the same\n    exception.\n\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"Thread.__init__() not called\")\n    if not self._started.is_set():\n        raise RuntimeError(\"cannot join thread before it is started\")\n    if self is current_thread():\n        raise RuntimeError(\"cannot join current thread\")\n\n    if timeout is None:\n        self._wait_for_tstate_lock()\n    else:\n        # the behavior of a negative timeout isn't documented, but\n        # historically .join(timeout=x) for x&lt;0 has acted as if timeout=0\n        self._wait_for_tstate_lock(timeout=max(timeout, 0))\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#threading.Thread.setDaemon","title":"<code>setDaemon(daemonic)</code>","text":"<p>Set whether this thread is a daemon.</p> <p>This method is deprecated, use the .daemon property instead.</p> Source code in <code>python3.10/threading.py</code> <pre><code>def setDaemon(self, daemonic):\n    \"\"\"Set whether this thread is a daemon.\n\n    This method is deprecated, use the .daemon property instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                  DeprecationWarning, stacklevel=2)\n    self.daemon = daemonic\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#threading.Thread.setName","title":"<code>setName(name)</code>","text":"<p>Set the name string for this thread.</p> <p>This method is deprecated, use the name attribute instead.</p> Source code in <code>python3.10/threading.py</code> <pre><code>def setName(self, name):\n    \"\"\"Set the name string for this thread.\n\n    This method is deprecated, use the name attribute instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('setName() is deprecated, set the name attribute instead',\n                  DeprecationWarning, stacklevel=2)\n    self.name = name\n</code></pre>"},{"location":"api/jax_dataloader/experimental/multi_processing/EpochIterator/#threading.Thread.start","title":"<code>start()</code>","text":"<p>Start the thread's activity.</p> <p>It must be called at most once per thread object. It arranges for the object's run() method to be invoked in a separate thread of control.</p> <p>This method will raise a RuntimeError if called more than once on the same thread object.</p> Source code in <code>python3.10/threading.py</code> <pre><code>def start(self):\n    \"\"\"Start the thread's activity.\n\n    It must be called at most once per thread object. It arranges for the\n    object's run() method to be invoked in a separate thread of control.\n\n    This method will raise a RuntimeError if called more than once on the\n    same thread object.\n\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"thread.__init__() not called\")\n\n    if self._started.is_set():\n        raise RuntimeError(\"threads can only be started once\")\n\n    with _active_limbo_lock:\n        _limbo[self] = self\n    try:\n        _start_new_thread(self._bootstrap, ())\n    except Exception:\n        with _active_limbo_lock:\n            del _limbo[self]\n        raise\n    self._started.wait()\n</code></pre>"},{"location":"api/jax_dataloader/loaders/base/BaseDataLoader/","title":"BaseDataLoader","text":""},{"location":"api/jax_dataloader/loaders/base/BaseDataLoader/#jax_dataloader.loaders.base.BaseDataLoader","title":"<code>jax_dataloader.loaders.base.BaseDataLoader</code>","text":"<p>Dataloader Interface</p> Source code in <code>jax_dataloader/loaders/base.py</code> <pre><code>class BaseDataLoader:\n    \"\"\"Dataloader Interface\"\"\"\n\n    def __init__(\n        self, \n        dataset, \n        batch_size: int = 1,  # batch size\n        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n        num_workers: int = 0,  # how many subprocesses to use for data loading.\n        drop_last: bool = False,\n        generator: Optional[GeneratorType] = None,\n        **kwargs\n    ):\n        pass\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __next__(self):\n        raise NotImplementedError\n\n    def __iter__(self):\n        raise NotImplementedError\n</code></pre>"},{"location":"api/jax_dataloader/loaders/base/BaseDataLoader/#jax_dataloader.loaders.base.BaseDataLoader-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/loaders/base/BaseDataLoader/#jax_dataloader.loaders.base.BaseDataLoader.__init__","title":"<code>__init__(dataset, batch_size: int = 1, shuffle: bool = False, num_workers: int = 0, drop_last: bool = False, generator: Optional[GeneratorType] = None, **kwargs)</code>","text":"Source code in <code>jax_dataloader/loaders/base.py</code> <pre><code>def __init__(\n    self, \n    dataset, \n    batch_size: int = 1,  # batch size\n    shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n    num_workers: int = 0,  # how many subprocesses to use for data loading.\n    drop_last: bool = False,\n    generator: Optional[GeneratorType] = None,\n    **kwargs\n):\n    pass\n</code></pre>"},{"location":"api/jax_dataloader/loaders/base/BaseDataLoader/#jax_dataloader.loaders.base.BaseDataLoader.__iter__","title":"<code>__iter__()</code>","text":"Source code in <code>jax_dataloader/loaders/base.py</code> <pre><code>def __iter__(self):\n    raise NotImplementedError\n</code></pre>"},{"location":"api/jax_dataloader/loaders/base/BaseDataLoader/#jax_dataloader.loaders.base.BaseDataLoader.__len__","title":"<code>__len__()</code>","text":"Source code in <code>jax_dataloader/loaders/base.py</code> <pre><code>def __len__(self):\n    raise NotImplementedError\n</code></pre>"},{"location":"api/jax_dataloader/loaders/base/BaseDataLoader/#jax_dataloader.loaders.base.BaseDataLoader.__next__","title":"<code>__next__()</code>","text":"Source code in <code>jax_dataloader/loaders/base.py</code> <pre><code>def __next__(self):\n    raise NotImplementedError\n</code></pre>"},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/","title":"DataLoaderTensorflow","text":""},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow","title":"<code>jax_dataloader.loaders.tensorflow.DataLoaderTensorflow</code>","text":"<p>               Bases: <code>BaseDataLoader</code></p> <p>Tensorflow Dataloader</p> Source code in <code>jax_dataloader/loaders/tensorflow.py</code> <pre><code>class DataLoaderTensorflow(BaseDataLoader):\n    \"\"\"Tensorflow Dataloader\"\"\"\n\n    @typecheck\n    def __init__(\n        self, \n        dataset: Union[JAXDataset, TFDataset, HFDataset],\n        batch_size: int = 1,  # Batch size\n        shuffle: bool = False,  # If true, dataloader shuffles before sampling each batch\n        drop_last: bool = False, # Drop last batch or not\n        generator: Optional[GeneratorType] = None, # Random seed generator\n        **kwargs\n    ):\n        super().__init__(dataset, batch_size, shuffle, drop_last)\n        check_tf_installed()\n        # get random seed from generator\n        seed = get_seed(generator)\n\n        # Convert to tf dataset\n        ds = to_tf_dataset(dataset)\n        ds = ds.shuffle(buffer_size=len(dataset), seed=seed) if shuffle else ds\n        ds = ds.batch(batch_size, drop_remainder=drop_last)\n        ds = ds.prefetch(tf.data.AUTOTUNE)\n        self.dataloader = ds\n\n    def __len__(self):\n        return len(self.dataloader)\n\n    def __next__(self):\n        return next(self.dataloader)\n\n    def __iter__(self):\n        return self.dataloader.as_numpy_iterator()\n</code></pre>"},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow.dataloader","title":"<code>dataloader = ds</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow.__init__","title":"<code>__init__(dataset: Union[JAXDataset, TFDataset, HFDataset], batch_size: int = 1, shuffle: bool = False, drop_last: bool = False, generator: Optional[GeneratorType] = None, **kwargs)</code>","text":"Source code in <code>jax_dataloader/loaders/tensorflow.py</code> <pre><code>@typecheck\ndef __init__(\n    self, \n    dataset: Union[JAXDataset, TFDataset, HFDataset],\n    batch_size: int = 1,  # Batch size\n    shuffle: bool = False,  # If true, dataloader shuffles before sampling each batch\n    drop_last: bool = False, # Drop last batch or not\n    generator: Optional[GeneratorType] = None, # Random seed generator\n    **kwargs\n):\n    super().__init__(dataset, batch_size, shuffle, drop_last)\n    check_tf_installed()\n    # get random seed from generator\n    seed = get_seed(generator)\n\n    # Convert to tf dataset\n    ds = to_tf_dataset(dataset)\n    ds = ds.shuffle(buffer_size=len(dataset), seed=seed) if shuffle else ds\n    ds = ds.batch(batch_size, drop_remainder=drop_last)\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    self.dataloader = ds\n</code></pre>"},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow.__iter__","title":"<code>__iter__()</code>","text":"Source code in <code>jax_dataloader/loaders/tensorflow.py</code> <pre><code>def __iter__(self):\n    return self.dataloader.as_numpy_iterator()\n</code></pre>"},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow.__len__","title":"<code>__len__()</code>","text":"Source code in <code>jax_dataloader/loaders/tensorflow.py</code> <pre><code>def __len__(self):\n    return len(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/loaders/tensorflow/DataLoaderTensorflow/#jax_dataloader.loaders.tensorflow.DataLoaderTensorflow.__next__","title":"<code>__next__()</code>","text":"Source code in <code>jax_dataloader/loaders/tensorflow.py</code> <pre><code>def __next__(self):\n    return next(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/","title":"DataLoaderPytorch","text":""},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch","title":"<code>jax_dataloader.loaders.torch.DataLoaderPytorch</code>","text":"<p>               Bases: <code>BaseDataLoader</code></p> <p>Pytorch Dataloader</p> Source code in <code>jax_dataloader/loaders/torch.py</code> <pre><code>class DataLoaderPytorch(BaseDataLoader):\n    \"\"\"Pytorch Dataloader\"\"\"\n\n    @typecheck\n    def __init__(\n        self, \n        dataset: Union[JAXDataset, TorchDataset, HFDataset],\n        batch_size: int = 1,  # Batch size\n        shuffle: bool = False,  # If true, dataloader shuffles before sampling each batch\n        drop_last: bool = False, # Drop last batch or not\n        generator: Optional[GeneratorType] = None,\n        **kwargs\n    ):\n        super().__init__(dataset, batch_size, shuffle, drop_last)\n        check_pytorch_installed()\n        from torch.utils.data import BatchSampler, RandomSampler, SequentialSampler\n        import torch\n\n        if 'sampler' in kwargs:\n            warnings.warn(\"`sampler` is currently not supported. We will ignore it and use `shuffle` instead.\")\n            del kwargs['sampler']\n\n        # convert to torch dataset\n        dataset = to_torch_dataset(dataset)\n        # init generator\n        if generator is None:\n            # explicitly set the manual seed of the generator\n            generator = Generator().manual_seed(get_config().global_seed)\n        if not isinstance(generator, Generator):\n            generator = Generator(generator=generator)\n\n        generator = generator.torch_generator()\n        # init batch sampler\n        if shuffle: \n            sampler = RandomSampler(dataset, generator=generator)\n        else:       \n            sampler = SequentialSampler(dataset)\n        batch_sampler = BatchSampler(sampler, batch_size=batch_size, drop_last=drop_last)\n\n        self.dataloader = torch_data.DataLoader(\n            dataset, \n            batch_sampler=batch_sampler,\n            # batch_size=batch_size, \n            # shuffle=shuffle, \n            # drop_last=drop_last,\n            collate_fn=_numpy_collate,\n            **kwargs\n        )\n\n    def __len__(self):\n        return len(self.dataloader)\n\n    def __next__(self):\n        return next(self.dataloader)\n\n    def __iter__(self):\n        return self.dataloader.__iter__()\n</code></pre>"},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch.dataloader","title":"<code>dataloader = torch_data.DataLoader(dataset, batch_sampler=batch_sampler, collate_fn=_numpy_collate, **kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch.__init__","title":"<code>__init__(dataset: Union[JAXDataset, TorchDataset, HFDataset], batch_size: int = 1, shuffle: bool = False, drop_last: bool = False, generator: Optional[GeneratorType] = None, **kwargs)</code>","text":"Source code in <code>jax_dataloader/loaders/torch.py</code> <pre><code>@typecheck\ndef __init__(\n    self, \n    dataset: Union[JAXDataset, TorchDataset, HFDataset],\n    batch_size: int = 1,  # Batch size\n    shuffle: bool = False,  # If true, dataloader shuffles before sampling each batch\n    drop_last: bool = False, # Drop last batch or not\n    generator: Optional[GeneratorType] = None,\n    **kwargs\n):\n    super().__init__(dataset, batch_size, shuffle, drop_last)\n    check_pytorch_installed()\n    from torch.utils.data import BatchSampler, RandomSampler, SequentialSampler\n    import torch\n\n    if 'sampler' in kwargs:\n        warnings.warn(\"`sampler` is currently not supported. We will ignore it and use `shuffle` instead.\")\n        del kwargs['sampler']\n\n    # convert to torch dataset\n    dataset = to_torch_dataset(dataset)\n    # init generator\n    if generator is None:\n        # explicitly set the manual seed of the generator\n        generator = Generator().manual_seed(get_config().global_seed)\n    if not isinstance(generator, Generator):\n        generator = Generator(generator=generator)\n\n    generator = generator.torch_generator()\n    # init batch sampler\n    if shuffle: \n        sampler = RandomSampler(dataset, generator=generator)\n    else:       \n        sampler = SequentialSampler(dataset)\n    batch_sampler = BatchSampler(sampler, batch_size=batch_size, drop_last=drop_last)\n\n    self.dataloader = torch_data.DataLoader(\n        dataset, \n        batch_sampler=batch_sampler,\n        # batch_size=batch_size, \n        # shuffle=shuffle, \n        # drop_last=drop_last,\n        collate_fn=_numpy_collate,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch.__iter__","title":"<code>__iter__()</code>","text":"Source code in <code>jax_dataloader/loaders/torch.py</code> <pre><code>def __iter__(self):\n    return self.dataloader.__iter__()\n</code></pre>"},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch.__len__","title":"<code>__len__()</code>","text":"Source code in <code>jax_dataloader/loaders/torch.py</code> <pre><code>def __len__(self):\n    return len(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/loaders/torch/DataLoaderPytorch/#jax_dataloader.loaders.torch.DataLoaderPytorch.__next__","title":"<code>__next__()</code>","text":"Source code in <code>jax_dataloader/loaders/torch.py</code> <pre><code>def __next__(self):\n    return next(self.dataloader)\n</code></pre>"},{"location":"api/jax_dataloader/tests/test_shuffle_reproducible/","title":"Test shuffle reproducible","text":""},{"location":"api/jax_dataloader/tests/test_shuffle_reproducible/#jax_dataloader.tests.test_shuffle_reproducible","title":"<code>jax_dataloader.tests.test_shuffle_reproducible(cls, ds, batch_size: int, feats, labels)</code>","text":"<p>Test that the shuffle is reproducible</p> Source code in <code>jax_dataloader/tests.py</code> <pre><code>def test_shuffle_reproducible(cls, ds, batch_size: int, feats, labels):\n    \"\"\"Test that the shuffle is reproducible\"\"\"\n    def _iter_dataloader(dataloader):\n        X_list, Y_list = [], []\n        for batch in dataloader:\n            x, y = get_batch(batch)\n            X_list.append(x)\n            Y_list.append(y)\n        return X_list, Y_list\n\n    # Test that the shuffle is reproducible\n    jdl.manual_seed(0)\n    dl_1 = cls(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n    X_list_1, Y_list_1 = _iter_dataloader(dl_1)\n    dl_2 = cls(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n    X_list_2, Y_list_2 = _iter_dataloader(dl_2)\n    assert jnp.array_equal(jnp.concatenate(X_list_1), jnp.concatenate(X_list_2))\n\n    # Test that the shuffle is different if the seed is different\n    jdl.manual_seed(1234)\n    dl_3 = cls(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n    X_list_3, Y_list_3 = _iter_dataloader(dl_3)\n    assert not jnp.array_equal(jnp.concatenate(X_list_1), jnp.concatenate(X_list_3))\n</code></pre>"},{"location":"api/jax_dataloader/utils/Config/","title":"Config","text":""},{"location":"api/jax_dataloader/utils/Config/#jax_dataloader.utils.Config","title":"<code>jax_dataloader.utils.Config</code>","text":"<p>Global configuration for the library</p> Source code in <code>jax_dataloader/utils.py</code> <pre><code>@dataclass\nclass Config:\n    \"\"\"Global configuration for the library\"\"\"\n    rng_reserve_size: int\n    global_seed: int\n\n    @classmethod\n    def default(cls) -&gt; Config:\n        return cls(rng_reserve_size=1, global_seed=42)\n</code></pre>"},{"location":"api/jax_dataloader/utils/Config/#jax_dataloader.utils.Config-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/utils/Config/#jax_dataloader.utils.Config.global_seed","title":"<code>global_seed: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/utils/Config/#jax_dataloader.utils.Config.rng_reserve_size","title":"<code>rng_reserve_size: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/utils/Generator/","title":"Generator","text":""},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator","title":"<code>jax_dataloader.utils.Generator</code>","text":"<p>A wrapper around JAX and PyTorch generators. This is used to generate random numbers in a reproducible way.</p> Source code in <code>jax_dataloader/utils.py</code> <pre><code>class Generator:\n    \"\"\"A wrapper around JAX and PyTorch generators. This is used to generate random numbers in a reproducible way.\"\"\"\n\n    def __init__(\n        self, \n        *, \n        generator: jax.Array | torch.Generator = None, # Optional generator\n    ):\n        self._seed = None\n        self._jax_generator = None\n        self._torch_generator = None\n\n        if generator is None:\n            self._seed = get_config().global_seed\n        elif (torch is not None) and isinstance(generator, torch.Generator):\n            self._torch_generator = generator\n        elif isinstance(generator, jax.Array):\n            self._jax_generator = generator\n        else:\n            raise ValueError(f\"generator=`{generator}` is invalid. Must be either a `jax.random.PRNGKey` or a `torch.Generator`.\")\n\n        if self._seed is None and self._torch_generator is not None:\n            self._seed = self._torch_generator.initial_seed()\n\n    def seed(self) -&gt; Optional[int]:\n        \"\"\"The initial seed of the generator\"\"\"\n        # TODO: the seed might not be initialized if the generator is a `jax.random.PRNGKey`\n        return self._seed\n\n    def manual_seed(self, seed: int) -&gt; Generator:\n        \"\"\"Set the seed for the generator. This will override the initial seed and the generator.\"\"\"\n\n        if self._jax_generator is not None:\n            self._jax_generator = jrand.PRNGKey(seed)\n        if self._torch_generator is not None:\n            self._torch_generator = torch.Generator().manual_seed(seed)\n        self._seed = seed\n        return self\n\n    def jax_generator(self) -&gt; jax.Array:\n        \"\"\"The JAX generator\"\"\"\n        if self._jax_generator is None:\n            self._jax_generator = jrand.PRNGKey(self._seed)\n        return self._jax_generator\n\n    def torch_generator(self) -&gt; torch.Generator:\n        \"\"\"The PyTorch generator\"\"\"\n        check_pytorch_installed()\n        if self._torch_generator is None and self._seed is not None:\n            self._torch_generator = torch.Generator().manual_seed(self._seed)\n        if self._torch_generator is None:\n            raise ValueError(\"Neither pytorch generator or seed is specified.\")\n        return self._torch_generator\n</code></pre>"},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator-attributes","title":"Attributes","text":""},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator._jax_generator","title":"<code>_jax_generator = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator._seed","title":"<code>_seed = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator._torch_generator","title":"<code>_torch_generator = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator-functions","title":"Functions","text":""},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator.__init__","title":"<code>__init__(*, generator: jax.Array | torch.Generator = None)</code>","text":"Source code in <code>jax_dataloader/utils.py</code> <pre><code>def __init__(\n    self, \n    *, \n    generator: jax.Array | torch.Generator = None, # Optional generator\n):\n    self._seed = None\n    self._jax_generator = None\n    self._torch_generator = None\n\n    if generator is None:\n        self._seed = get_config().global_seed\n    elif (torch is not None) and isinstance(generator, torch.Generator):\n        self._torch_generator = generator\n    elif isinstance(generator, jax.Array):\n        self._jax_generator = generator\n    else:\n        raise ValueError(f\"generator=`{generator}` is invalid. Must be either a `jax.random.PRNGKey` or a `torch.Generator`.\")\n\n    if self._seed is None and self._torch_generator is not None:\n        self._seed = self._torch_generator.initial_seed()\n</code></pre>"},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator.jax_generator","title":"<code>jax_generator() -&gt; jax.Array</code>","text":"<p>The JAX generator</p> Source code in <code>jax_dataloader/utils.py</code> <pre><code>def jax_generator(self) -&gt; jax.Array:\n    \"\"\"The JAX generator\"\"\"\n    if self._jax_generator is None:\n        self._jax_generator = jrand.PRNGKey(self._seed)\n    return self._jax_generator\n</code></pre>"},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator.manual_seed","title":"<code>manual_seed(seed: int) -&gt; Generator</code>","text":"<p>Set the seed for the generator. This will override the initial seed and the generator.</p> Source code in <code>jax_dataloader/utils.py</code> <pre><code>def manual_seed(self, seed: int) -&gt; Generator:\n    \"\"\"Set the seed for the generator. This will override the initial seed and the generator.\"\"\"\n\n    if self._jax_generator is not None:\n        self._jax_generator = jrand.PRNGKey(seed)\n    if self._torch_generator is not None:\n        self._torch_generator = torch.Generator().manual_seed(seed)\n    self._seed = seed\n    return self\n</code></pre>"},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator.seed","title":"<code>seed() -&gt; Optional[int]</code>","text":"<p>The initial seed of the generator</p> Source code in <code>jax_dataloader/utils.py</code> <pre><code>def seed(self) -&gt; Optional[int]:\n    \"\"\"The initial seed of the generator\"\"\"\n    # TODO: the seed might not be initialized if the generator is a `jax.random.PRNGKey`\n    return self._seed\n</code></pre>"},{"location":"api/jax_dataloader/utils/Generator/#jax_dataloader.utils.Generator.torch_generator","title":"<code>torch_generator() -&gt; torch.Generator</code>","text":"<p>The PyTorch generator</p> Source code in <code>jax_dataloader/utils.py</code> <pre><code>def torch_generator(self) -&gt; torch.Generator:\n    \"\"\"The PyTorch generator\"\"\"\n    check_pytorch_installed()\n    if self._torch_generator is None and self._seed is not None:\n        self._torch_generator = torch.Generator().manual_seed(self._seed)\n    if self._torch_generator is None:\n        raise ValueError(\"Neither pytorch generator or seed is specified.\")\n    return self._torch_generator\n</code></pre>"},{"location":"api/jax_dataloader/utils/manual_seed/","title":"Manual seed","text":""},{"location":"api/jax_dataloader/utils/manual_seed/#jax_dataloader.utils.manual_seed","title":"<code>jax_dataloader.utils.manual_seed(seed: int)</code>","text":"<p>Set the seed for the library</p> Source code in <code>jax_dataloader/utils.py</code> <pre><code>def manual_seed(seed: int):\n    \"\"\"Set the seed for the library\"\"\"\n    main_config.global_seed = seed\n</code></pre>"},{"location":"examples/random/","title":"Controlling Randomness","text":"<p><code>jax-dataloader</code> provides flexible mechanisms to manage the pseudo-random number generation used during data loading, which is essential for reproducibility, especially when shuffling data. This tutorial outlines the two primary ways to control randomness:</p> <ul> <li>Setting a global seed</li> <li>Assigning specific seed generators to individual dataloaders.</li> </ul>"},{"location":"examples/random/#prerequisites","title":"Prerequisites","text":"<p>Let\u2019s set up the necessary imports and a simple dataset for our examples:</p> <pre><code>import jax_dataloader as jdl\nimport jax\nimport jax.numpy as jnp\nimport torch\n\n# Sample dataset\ndata = jnp.arange(20).reshape(10, 2)\nlabels = jnp.arange(10)\nds = jdl.ArrayDataset(data, labels)\n</code></pre>"},{"location":"examples/random/#method-1-setting-the-global-seed","title":"Method 1: Setting the Global Seed","text":"<p>The simplest way to control randomness across all <code>jax-dataloader</code> instances is by setting a global seed. This affects all dataloaders created after the seed is set, unless they have their own specific generator specified.</p> <p>Use the <code>jax_dataloader.manual_seed()</code> function:</p> <pre><code># Set the global seed for all subsequent dataloaders\njdl.manual_seed(1234)\n\n# Both dataloaders below will use the same underlying seed sequence\n# resulting in identical shuffling order if other parameters are the same.\ndl_1 = jdl.DataLoader(ds, backend='jax', batch_size=2, shuffle=True)\ndl_2 = jdl.DataLoader(ds, backend='jax', batch_size=2, shuffle=True)\n\n# Iterate through dl_1 and dl_2 to observe the same order\nprint(\"DataLoader 1 first batch:\", next(iter(dl_1)))\nprint(\"DataLoader 2 first batch:\", next(iter(dl_2)))\n</code></pre> <pre><code>DataLoader 1 first batch: (array([[2, 3],\n       [4, 5]], dtype=int32), array([1, 2], dtype=int32))\nDataLoader 2 first batch: (array([[2, 3],\n       [4, 5]], dtype=int32), array([1, 2], dtype=int32))\n</code></pre>"},{"location":"examples/random/#method-2-setting-per-dataloader-seed-generators","title":"Method 2: Setting Per-Dataloader Seed Generators","text":"<p>For more fine-grained control, assign a specific seed generator to individual DataLoader instances using the generator argument. This overrides any global seed for that specific dataloader.</p> <p>jax-dataloader supports generators from <code>jax-dataloader</code>, <code>jax.random.PRNGKey</code>, and <code>torch.Generator</code>.</p>"},{"location":"examples/random/#1-using-jdlgenerator","title":"1. Using <code>jdl.Generator</code>","text":"<p>Create and seed a <code>jdl.Generator</code> object and pass it to the <code>jdl.DataLoader</code>.</p> <pre><code># Create a specific generator with its own seed\ng1 = jdl.Generator().manual_seed(4321)\n\n# This dataloader will use g1, overriding any global seed\ndl_jdl_gen = jdl.DataLoader(ds, backend='jax', batch_size=2, shuffle=True, generator=g1)\n\nprint(\"DataLoader with jdl.Generator first batch:\", next(iter(dl_jdl_gen)))\n</code></pre> <pre><code>DataLoader with jdl.Generator first batch: (array([[ 6,  7],\n       [10, 11]], dtype=int32), array([3, 5], dtype=int32))\n</code></pre>"},{"location":"examples/random/#2-using-jaxrandomprngkey","title":"2. Using <code>jax.random.PRNGKey</code>","text":"<p>Directly use a <code>jax.random.PRNGKey</code> as the generator.</p> <pre><code># Create a JAX PRNGKey\nkey = jax.random.PRNGKey(4321)\n\n# This dataloader will use the JAX key, overriding any global seed\n# jax-dataloader handles the key internally for reproducible iteration.\ndl_jax_key = jdl.DataLoader(ds, backend='jax', batch_size=2, shuffle=True, generator=key)\n\nprint(\"DataLoader with JAX PRNGKey first batch:\", next(iter(dl_jax_key)))\n</code></pre> <pre><code>DataLoader with JAX PRNGKey first batch: (array([[ 6,  7],\n       [10, 11]], dtype=int32), array([3, 5], dtype=int32))\n</code></pre>"},{"location":"examples/random/#3-using-torchgenerator","title":"3. Using torch.Generator","text":"<p>When using the <code>'torch'</code> backend, you can use a <code>torch.Generator</code>.</p> <pre><code># Create a PyTorch generator\ng3 = torch.Generator().manual_seed(5678)\n\n# This dataloader uses the 'torch' backend and the PyTorch generator\ndl_torch_gen = jdl.DataLoader(ds, backend='pytorch', batch_size=2, shuffle=True, generator=g3)\n\nprint(\"DataLoader with torch.Generator first batch:\", next(iter(dl_torch_gen)))\n</code></pre> <pre><code>DataLoader with torch.Generator first batch: [array([[ 0,  1],\n       [14, 15]], dtype=int32), array([0, 7], dtype=int32)]\n</code></pre>"},{"location":"examples/random/#trade-offs-global-seed-vs-per-dataloader-generators","title":"Trade-offs: Global Seed vs.\u00a0Per-Dataloader Generators","text":"<p>Consider these trade-offs when deciding how to manage randomness.</p>"},{"location":"examples/random/#global-seed-jdlmanual_seed","title":"Global Seed (<code>jdl.manual_seed()</code>)","text":"<ul> <li>Simplicity: Very easy to implement with one line for basic   reproducibility.</li> <li>Implicit Consistency: Automatically ensures dataloaders created   subsequently (without their own generator) share the same base   randomness, useful for simple synchronization.</li> </ul>"},{"location":"examples/random/#per-dataloader-generator-generator","title":"Per-Dataloader Generator (<code>generator=...</code>)","text":"<ul> <li>Fine-grained Control: Allows independent and precise randomness   management for each dataloader.</li> <li>Isolation: Prevents randomness in one dataloader from affecting   others.</li> <li>Integration: Works naturally with JAX keys or PyTorch generators.</li> <li>Modularity: Better suited for complex applications or libraries where   components need self-contained randomness.</li> </ul>"},{"location":"examples/torch_mnist/","title":"Torch mnist","text":"<pre><code>import jax_dataloader as jdl\nimport haiku as hk\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport optax\nfrom functools import partial\nimport time\n</code></pre> <pre><code>from torchvision.datasets import FashionMNIST\nfrom torch.utils.data import DataLoader\n</code></pre> <pre><code>from datasets import load_dataset\n</code></pre> <pre><code>import tensorflow_datasets as tfds\n</code></pre> <pre><code>class FlattenAndCast(object):\n  def __call__(self, pic):\n    return np.array(pic, dtype=float)\n</code></pre> <pre><code>def net_fn(imgs: jnp.ndarray):\n    B, H, W = imgs.shape\n    imgs = imgs.reshape(B, H, W, 1)\n    x = imgs.astype(jnp.float32) / 255.\n    cov = hk.Sequential([\n        hk.Conv2D(32, 3, 2),\n        jax.nn.relu,\n        hk.Conv2D(64, 3, 2),\n        jax.nn.relu,\n        hk.Conv2D(128, 3, 2),\n        jax.nn.relu,\n        hk.Flatten(),\n        hk.Linear(256),\n        jax.nn.relu,\n        hk.Linear(10),\n    ])\n    return cov(x)\n\n\noptax_cross_entropy = optax.softmax_cross_entropy_with_integer_labels\n\ndef loss(\n    params: hk.Params, \n    classifier: hk.Transformed, \n    imgs: jnp.ndarray, \n    labels: jnp.ndarray\n):\n    logits = classifier.apply(params, imgs)\n    return jnp.mean(jax.vmap(optax_cross_entropy)(logits, labels=labels))\n</code></pre> <pre><code>def init():\n    classifier = hk.without_apply_rng(hk.transform(net_fn))\n    opt = optax.adam(1e-3)\n    params = classifier.init(jax.random.PRNGKey(42), jnp.ones((32, 28, 28)))\n    opt_state = opt.init(params)\n    return classifier, opt, params, opt_state\n</code></pre> <pre><code># @jax.jit\n@partial(jax.jit, static_argnums=(2,3))\ndef update(\n    params: hk.Params,\n    opt_state: optax.OptState,\n    classifier: hk.Transformed,\n    opt: optax.GradientTransformation,\n    imgs: jnp.ndarray,\n    labels: jnp.ndarray\n):\n    grads = jax.grad(loss)(params, classifier, imgs, labels)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state\n</code></pre> <pre><code>def _numpy_collate(batch):\n    if isinstance(batch[0], (np.ndarray, jax.Array)):\n        return np.stack(batch)\n    elif isinstance(batch[0], (tuple, list)):\n        transposed = zip(*batch)\n        return [_numpy_collate(samples) for samples in transposed]\n    elif isinstance(batch[0], dict):\n        return {key: _numpy_collate([d[key] for d in batch]) for key in batch[0]}\n    else:\n        return np.array(batch)\n</code></pre> <pre><code>def get_img_labels(batch):\n    if isinstance(batch, tuple) or isinstance(batch, list):\n        # print(batch[0])\n        if isinstance(batch[0], dict):\n            imgs, labels = batch[0]['image'], batch[0]['label']\n        else:\n            imgs, labels = batch\n    elif isinstance(batch, dict):\n        imgs, labels = batch['image'], batch['label']\n    else:\n        raise ValueError(f'Unknown batch type: {type(batch)}', )\n    return imgs, labels\n\ndef train(\n    train_ds,\n    backend: str,\n    batch_size: int,\n    shuffle: bool = True,\n    n_epochs: int = 1\n):\n    train_dl = jdl.DataLoader(\n        train_ds, backend=backend, batch_size=batch_size, shuffle=shuffle)\n    imgs_list= []\n    classifier, opt, params, opt_state = init()\n\n    train_start_time = time.time()\n    for i in range(n_epochs):\n        epoch_start_time = time.time()\n        for batch in train_dl:\n            imgs, labels = get_img_labels(batch)\n\n            params, opt_state = update(\n                params, opt_state, classifier, opt, imgs, labels)\n            imgs_list.append(imgs)\n\n        epoch_time = time.time() - epoch_start_time\n        print(f'Epoch {i} took {epoch_time: .3f} seconds')\n        print(f'Per batch: {epoch_time / len(train_dl): .3f} seconds')\n\n    train_time = time.time() - train_start_time\n    print(f'Training took {train_time: .3f} seconds')\n\n    imgs_list = jnp.concatenate(imgs_list)\n    assert imgs_list.shape == (len(train_ds), 28, 28)\n    return train_time\n</code></pre> <pre><code>train_ds_torch = FashionMNIST(\n    '/tmp/mnist/', download=True, transform=FlattenAndCast(), train=True)\ntest_ds_torch = FashionMNIST(\n    '/tmp/mnist/', download=True, transform=FlattenAndCast(), train=False)\n\ntrain_ds_jax = jdl.ArrayDataset(\n    train_ds_torch.data.numpy(), train_ds_torch.targets.numpy())\ntest_ds_jax = jdl.ArrayDataset(\n    test_ds_torch.data.numpy(), test_ds_torch.targets.numpy())\n</code></pre> <pre><code>train_ds_hf = load_dataset('fashion_mnist', split='train')\ntest_ds_hf = load_dataset('fashion_mnist', split='test')\n</code></pre> <pre><code>Downloading builder script:   0%|          | 0.00/4.83k [00:00&lt;?, ?B/s]\n\nDownloading data files:   0%|          | 0/4 [00:00&lt;?, ?it/s]\n\nExtracting data files:   0%|          | 0/4 [00:00&lt;?, ?it/s]\n\nGenerating train split:   0%|          | 0/60000 [00:00&lt;?, ? examples/s]\n\nGenerating test split:   0%|          | 0/10000 [00:00&lt;?, ? examples/s]\n</code></pre> <pre><code>train_ds_tf = tfds.load('fashion_mnist', split='train')\ntest_ds_tf = tfds.load('fashion_mnist', split='test')\n</code></pre> <pre><code># train(train_ds_torch, 'pytorch', 128)\n# train(train_ds_jax, 'jax', 128)\n# train(train_ds_jax, 'pytorch', 128)\n# train(train_ds_hf, 'jax', 128)\n# train(train_ds_hf.with_format('jax'), 'pytorch', 128)\n# train(train_ds_tf, 'tensorflow', 128)\n</code></pre>"},{"location":"examples/vit.torch/","title":"Vision Transformer","text":"<pre><code>import jax\nfrom jax import numpy as jnp, random as jrand, tree_util as jt\nimport optax\nimport numpy as np\nimport einops\n\n# nn specific\nimport flax.linen as nn\n\n# data specific\nimport jax_dataloader as jdl\nimport torchvision\n\n# utils\nimport functools as ft\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom tqdm.auto import tqdm\n</code></pre> <pre><code>transforms = torchvision.transforms\n\nclass ToNumpy:\n    def __call__(self, x): # (H, W, C)\n        return np.array(x) / 255.0\n\n@dataclass\nclass Normalize:\n    mean: list[float] \n    std: list[float]\n    inplace: bool = False\n\n    def __call__(self, x: np.ndarray):\n        if x.ndim &lt; 3:\n            raise ValueError(\n                f\"Expected tensor to be a tensor image of size (..., C, H, W). \"\n                f\"Got x.shape = {x.shape}\"\n            )\n        if not self.inplace:\n            x = x.copy()\n\n        dtype = x.dtype\n        mean = np.asarray(self.mean, dtype=dtype)\n        std = np.asarray(self.std, dtype=dtype)\n        if np.any(self.std) == 0:\n            raise ValueError(f\"std evaluated to zero after conversion to {dtype}, \"\n                             f\"leading to division by zero.\")\n        if mean.ndim == 1:\n            mean = einops.rearrange(mean, 'C -&gt; 1 1 C')\n        if std.ndim == 1:\n            std = einops.rearrange(std, 'C -&gt; 1 1 C')\n\n        return (x - mean) / std\n</code></pre> <pre><code>class PositionalEmbedding(nn.Module):\n    dtype = jnp.float32\n\n    @nn.compact\n    def __call__(self, x):\n        batch_size, seq_len, emb_dim = x.shape\n        pos_emb_shape = (1, seq_len, emb_dim)\n        pe = self.param('positional_embedding', \n                        nn.initializers.normal(stddev=0.02), pos_emb_shape)\n        return x + pe\n</code></pre> <pre><code>class PatchEmbedding(nn.Module):\n    num_hiddens: int\n    dtype = jnp.float32\n\n    @nn.compact\n    def __call__(self, x: jax.Array):\n        x = einops.rearrange(\n            x, \"... (H PH) (W PW) C -&gt; ... (H W) (PH PW C)\",\n            PH=patch_size, PW=patch_size\n        )\n        x = nn.Dense(self.num_hiddens, dtype=self.dtype)(x)\n        return x\n</code></pre> <pre><code>class MLP(nn.Module):\n    hidden_dim: int\n    dropout_rate: float\n    dtype = jnp.float32\n\n    @nn.compact\n    def __call__(self, x, train):\n        out_dim = x.shape[-1]\n        x = nn.Dense(self.hidden_dim, \n                     kernel_init=nn.initializers.xavier_uniform())(x)\n        x = nn.gelu(x)\n        x = nn.Dropout(self.dropout_rate, deterministic=not train)(x)\n        x = nn.Dense(out_dim, dtype=self.dtype,\n                     kernel_init=nn.initializers.xavier_uniform())(x)\n        x = nn.Dropout(self.dropout_rate, deterministic=not train)(x)\n        return x\n</code></pre> <pre><code>class EncoderBlock(nn.Module):\n    mlp_dim: int\n    num_heads: int\n    dropout_rate: float = 0.1\n    attention_dropout_rate: float = 0.1\n    dtype = jnp.float32\n\n    @nn.compact\n    def __call__(self, inputs, train):\n        x = nn.LayerNorm()(inputs)\n        x = nn.MultiHeadDotProductAttention(\n            num_heads=self.num_heads,\n            dropout_rate=self.attention_dropout_rate,\n            deterministic=not train,\n            kernel_init=nn.initializers.xavier_uniform()\n        )(x)\n        x = nn.Dropout(self.dropout_rate, deterministic=not train)(x)\n        x = x + inputs\n\n        # mlp\n        y = nn.LayerNorm()(x)\n        y = MLP(self.mlp_dim, self.dropout_rate)(y, train)\n        return x + y\n</code></pre> <pre><code>class ViT(nn.Module):\n    num_classes: int\n    num_layers: int\n    hidden_dim: int\n    num_heads: int\n    mlp_dim: int\n    dropout_rate: float\n    attention_dropout_rate: float\n    dtype = jnp.float32\n\n    @nn.compact\n    def __call__(self, x, train):\n        B, H, W, C = x.shape\n\n        x = PatchEmbedding(self.hidden_dim)(x)\n        cls_token = self.param('cls_token', \n                               nn.initializers.normal(stddev=0.02), \n                               (1, 1, self.hidden_dim))\n        cls_token = jnp.tile(cls_token, (B, 1, 1)) # (B, 1, hidden_dim)\n        x = jnp.concatenate([cls_token, x], axis=1)\n\n        x = PositionalEmbedding()(x)\n        x = nn.Dropout(self.dropout_rate, deterministic=not train)(x)\n        for _ in range(self.num_layers):\n            x = EncoderBlock(self.mlp_dim, self.num_heads, \n                             self.dropout_rate, self.attention_dropout_rate)(x, train)\n        x = nn.LayerNorm()(x)\n        x = x[:, 0]\n        x = nn.Dense(self.num_classes)(x)\n        return x\n</code></pre> <pre><code>@jax.value_and_grad\ndef compute_grad(\n    params,\n    model: nn.Module,\n    batch: tuple[jnp.ndarray, jnp.ndarray],\n    key: jrand.PRNGKey,\n):\n    img, label = batch\n    logits = model.apply(params, img, rngs={'dropout': key}, train=True)\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits, label)\n    return loss.mean()\n\n@ft.partial(jax.jit, static_argnums=(1, 2))\ndef step(\n    params,\n    model: nn.Module,\n    opt: optax.GradientTransformation,\n    opt_state: optax.OptState,\n    batch: tuple[jnp.ndarray, jnp.ndarray],\n    key: jrand.PRNGKey,\n):\n    loss, grads = compute_grad(params, model, batch, key)\n    updates, opt_state = opt.update(grads, opt_state, params)\n    new_params = optax.apply_updates(params, updates)\n    return new_params, opt_state, loss\n\ndef train(\n    model: nn.Module,\n    optimizer: optax.GradientTransformation,\n    data_loader: jdl.DataLoader,\n    epochs: int,\n    rng_key: jrand.PRNGKey = jrand.PRNGKey(0),\n):\n    rng_key, init_key = jrand.split(rng_key)\n    xs, ys = next(iter(data_loader))\n    params = model.init(init_key, xs, train=False)\n    opt_state = optimizer.init(params)\n    losses, steps = [], 0\n\n    for epoch in range(epochs):\n        for batch in data_loader:\n            rng_key, key = jrand.split(rng_key)\n            params, opt_state, loss = step(\n                params, model, optimizer, opt_state, batch, key\n            )\n            losses.append(loss)\n            steps += 1\n\n            if steps % 500 == 0:\n                print(f\"Epoch: {epoch}, Step: {steps}, Loss: {loss}\")\n    return params, losses\n</code></pre> <pre><code># Hyperparameters\nlr = 3e-4\ndropout_rate = 0.1\nbeta1 = 0.9\nbeta2 = 0.99\nbatch_size = 64 * 2 * 2\npatch_size = 4\nnum_patches = 64\nnum_steps = 100000\nimage_size = (32, 32, 3)\nembedding_dim = 512\nhidden_dim = 256\nnum_heads = 8\nnum_layers = 4\nheight, width, channels = image_size\nnum_classes = 10\n</code></pre> <pre><code>transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.Resize((height, width)),\n        transforms.RandomHorizontalFlip(),\n        ToNumpy(),\n        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n        transforms.Resize((height, width)),\n        ToNumpy(),\n        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(\n    \"/tmp/CIFAR/\",\n    train=True,\n    download=True,\n    transform=transform_train,\n\n)\n\ntest_dataset = torchvision.datasets.CIFAR10(\n    \"/tmp/CIFAR/\",\n    train=False,\n    download=True,\n    transform=transform_test,\n)\n</code></pre> <pre><code>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/CIFAR/cifar-10-python.tar.gz\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170498071/170498071 [00:02&lt;00:00, 63361930.39it/s]\n\nExtracting /tmp/CIFAR/cifar-10-python.tar.gz to /tmp/CIFAR/\nFiles already downloaded and verified\n</code></pre> <pre><code>vit = ViT(\n    num_classes=num_classes,\n    num_layers=num_layers,\n    hidden_dim=hidden_dim,\n    num_heads=num_heads,\n    mlp_dim=embedding_dim,\n    dropout_rate=dropout_rate,\n    attention_dropout_rate=dropout_rate,\n)\nschedule_fn = optax.warmup_cosine_decay_schedule(\n    init_value=0.0, peak_value=lr, warmup_steps=500, decay_steps=10_000\n)\nopt = optax.chain(\n    optax.clip_by_global_norm(1.0),\n    optax.adamw(learning_rate=schedule_fn, b1=beta1, b2=beta2),\n)\ndl = jdl.DataLoader(train_dataset, 'pytorch', batch_size=batch_size, shuffle=True)\n</code></pre> <pre><code>params, losses = train(vit, opt, dl, 500)\n</code></pre> <pre><code>corrects = []\n\ndl = jdl.DataLoader(test_dataset, 'pytorch', batch_size=batch_size * 4, shuffle=True)\nfor batch in dl:\n    img, label = batch\n    logits = vit.apply(params, img, rngs={'dropout': jrand.PRNGKey(0)}, train=False)\n    preds = jnp.argmax(logits, axis=-1)\n    corrects.append((preds == label))\n\nprint(f\"Accuracy: {np.concatenate(corrects).mean()}\")\n</code></pre> <pre><code>Accuracy: 0.8149\n</code></pre>"},{"location":"experimental/mp/","title":"Mp","text":"<p>source</p>"},{"location":"experimental/mp/#chunk","title":"chunk","text":"<p>chunk (seq:Sequence, size:int)</p> <p>source</p>"},{"location":"experimental/mp/#epochiterator","title":"EpochIterator","text":"<p>EpochIterator (data, batch_size:int, indices:Sequence[int])</p> <p>[WIP] Multiprocessing Epoch Iterator</p>"}]}